[
  {
    "objectID": "posts/continual-learning-beginners-guide.html",
    "href": "posts/continual-learning-beginners-guide.html",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "I am currently researching on continual learning, a paradigm of machine learning. I am very keen to introduce its basic knowledge here to you whoever insterested in the area or just in what I’m doing personally. I’ll make this post into an index page for resources when more continual learning comes up.\nIn this post I’ll explain the basics of continual learning including concepts, problem definition, datasets, metrics, etc, and go through some selected classic algorithms in each of their method categories."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#the-scope",
    "href": "posts/continual-learning-beginners-guide.html#the-scope",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "1.1 The Scope",
    "text": "1.1 The Scope\nFirst things first, it’s very important to understand the scope of this concept: continual learning is one of the machine learning paradigms.\nSo what are those machine learning paradigms? We always come across a bunch of different terms when looking up for machine learning, such as supervised / unsupervised learning, reinforcement learning, transfer learning, or perhaps image classification, object detection, machine translation if you are more insterested in real applications. There are lots of mixed usages among the words like scenarios, tasks, settings, problems, but here we do need a clear definition in these terms. We call the real applications as scenarios, and the more abstract ones as paradigms. Formally, paradigms are more of categories about how the data distribute and the way that they are allowed to be used, and how the model is evaluated, without looking into what types of data are.\nAs we mentioned, supervised / unsupervised learning is probably the most frequently involved paradigms. They are about general components of the data – if labels are used for training, and usually an one-off training and testing process, which we often refer to as a task. Over the past decade, many problems that involve multiple tasks got into sights of machine learning research and turned into paradigms which led to increasing popularity in deep learning research keywords. They include but are not limited to transfer learning, multi-task learning, online learning, meta learning, and of course, continual learning."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#definition",
    "href": "posts/continual-learning-beginners-guide.html#definition",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "1.2 Definition",
    "text": "1.2 Definition\nIn the concept level I define continual learning as this:\n\nContinual learning is a multi-task machine learning paradigm where an algorithm receives the data from tasks sequentially without the access to previous ones to learn a model that performs the best for all tasks.\n\nPlease be aware that “sequentially” and “for all tasks” are both essential to this concept. It could turn to other paradigms without either of them. See “Difference From Other Paradigms” for details.\nThe non-accessibility to previous task is also the key to differentiate CL. It is a very practical assumption, considering the huge memory cost or potential violence to privacy to store all previous data in real-world scenarios.\nIf previous data are allowed to be accessed, the model can be retrained with all data from new task and previous tasks, which is sometimes called joint training. However, it is not even better. Other than the computational cost, the data from various distributions make learning difficult as well. The model might learn something too general to encourage benefits for all, or even something wrong (sometimes called induction bias). Multi-task learning paradigm tries to address this problem in some ways, but again, it doesn’t worth to take a detour with training a multi-task model for mutiple times.\nWe must also notice that continual learning deals with infinite sequence of tasks so the algorithm has no idea about its future challenges. That is what “continual” is meant for.\nIt’s also important to note that continual learning typically deals with tasks that vary significantly from each other. If the tasks were from the same distribution or highly similar, it wouldn’t be a challenge for the algorithm to perform well naturally across all of them. In other words, the data stream is usually non-stationary (supposing we blur the boundaries between tasks).\nThere are smaller categories in continual learning such as TIL and CIL, but it cannot be explained abstractly. We’ll come to that later after introducing classification."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#differences-from-other-paradigms",
    "href": "posts/continual-learning-beginners-guide.html#differences-from-other-paradigms",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "1.3 Differences From Other Paradigms",
    "text": "1.3 Differences From Other Paradigms\nThe differences from other paradigms, again, consider the data used (training) and evaluated (testing). We have an inspirational illustration from the paper:\n\n\nStandard supervised learning: continual learning with one task reduces itself to this normal machine learning paradigm.\nMulti-task learning: If continual learning is without “sequentially”, the algorithm has access to data of each task all at once when training the last task. It turns to multi-task learning.\nTransfer learning: If continual learning is without “for all tasks”: like if the performance for the last task alone becomes what the algorithm aims for, it turns like transfer learning. If for other task but the last, it turns like a less common paradigm called reverse transfer learning. Note that transfer learning is usually a two-task (A & B) paradigm. Domain adaptation is something similar to transfer learning.\nOnline learning: it is actually a single-task paradigm where the test and apply are as frequent as the training data. It does look like continual learning though but they are completely different because the online learning data are from the same distribution.\nMeta learning: If “for all tasks” in continual learning definition turns to “for unseen new tasks”, it turns to meta learning. In this case the algorithm cannot get any supervising info of the unseen tasks so it has to learn to learn. This idea is conducted by a meta learner which considers tasks as samples for learning to learn, in a meta level."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#why-continual",
    "href": "posts/continual-learning-beginners-guide.html#why-continual",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "1.4 Why Continual?",
    "text": "1.4 Why Continual?\nThere are many features in human learning. One of the most important is to learn and adapt new knowledge continuously without forgetting previous knowledge. Unfortunately, it can hardly be achieved through standard deep learning process, therefore a wide range of paradigms aformentioned are explored from different aspects to equip AI not only with a huge performance on a single task. Continual learning is the paradigm primarily oriented towards addressing the problem of forgetting in neural networks. (The problem is generally refered to as catastrophic forgetting. We’ll come to that later. )\nContinual learning is still in its early age without so many examples of applications due to its difficulty, but it is a highly potential solution to any real-world applications facing a continuous stream of non-stationary data when it’s a bad idea to retrain from scratch. Here are some examples:\n\nIn Robotics\n\nRobotic agents are naturally playground for continual learning because of they interact with real world, and some might say CL is born for robotics. There are various scenarios like object detection, segmentation, reinforcement learning which face the non-stationary data challenges1.\n\nIn Autonomous Driving\n\nThe environment and driving conditions are constantly changing, like weather, traffic, objects2 3.\n\nIn Finance\n\nFor example, anomaly detection in auditing4: a company might face different patterns of frauds in their financial quarters or years. Other applications could be like algorithm trading, portfolio selection5, financial forecasting6, credit scoring.\n\nIt can also fit in other scenarios like CV, NLP, recommendation systems, health care, etc."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#task-incremental-learning-til",
    "href": "posts/continual-learning-beginners-guide.html#task-incremental-learning-til",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "2.1 Task-Incremental Learning (TIL)",
    "text": "2.1 Task-Incremental Learning (TIL)\nTask-Incremental Learning (TIL) gives model the information of which task the instance is from during testing, i.e. the test instance is \\((\\mathbf{x}, y, t)\\) where we know \\((\\mathbf{x}, y) \\in (\\mathcal{X}^{(t)},\\mathcal{Y}^{(t)})\\). This is similar to how A-level exams are conducted separately for each subject, with students knowing perfectly well which subject they are being tested on.\nIn that case, the output heads for different tasks can be totally segregated. As shown in ?@fig-TILvsCIL, a TIL head are made up for its new task alone because it doesn’t have to consider about other tasks.\nIf the task ID information of test instances is not given, it becomes a more difficult paradigm, which I call as task-agnostic testing (though it is not much commonly used). In this case, the model has to figure out the test ID by itself.\nThere are even more advanced paradigms which eliminate the task boundary. For example, task-agnostic continual learning assumes task IDs are not even given during training. We won’t cover these paradigms."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#class-incremental-learning-cil",
    "href": "posts/continual-learning-beginners-guide.html#class-incremental-learning-cil",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "2.2 Class-Incremental Learning (CIL)",
    "text": "2.2 Class-Incremental Learning (CIL)\nClass-Incremental Learning (CIL) mixes classes from all tasks together and let the model choose from a class that could be from any seen task, i.e. the test instance is \\((\\mathbf{x}, y) \\in (\\mathcal{X}^{(1)}\\cup \\cdots\\cup \\mathcal{X}^{(t)},\\mathcal{Y}^{(1)}\\cup \\cdots\\cup \\mathcal{Y}^{(t)})\\). You can imagine a huge session of A-level exam that include and mix all the subjects in one single paper. It drops the assumption that the model knows task ID, and make itself much more difficult than TIL, especially when the task squence goes long.\nThe output heads therefore evolve incrementally as shown in ?@fig-TILvsCIL. The model would use the last huge output head which inherit all the previous heads to test, instead of finding the independent head of a known task ID."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#dataset-permute-split-combine",
    "href": "posts/continual-learning-beginners-guide.html#dataset-permute-split-combine",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "3.1 Dataset: Permute, Split, Combine",
    "text": "3.1 Dataset: Permute, Split, Combine\nThe datasets where continual learning algorithms are measured are sequences of normal machine learning datasets. They are usually constructed in three ways:\n\nCombine: from different sources of ML datasets, each serving as a task. For example, HAT evaluated on a sequence from 8 datasets: CIFAR10 and CIFAR100 (Krizhevsky, 2009), FaceScrub (Ng & Winkler, 2014), FashionMNIST (Xiao et al., 2017), NotMNIST (Bulatov, 2011), MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), and TrafficSigns. Note that the datasets might have different input dimensions, so either align the inputs or properly design task-specific model inputs\nPermute: there are ways to construct from one datasets. We can permute the image in a certain way from a dataset to get a different task.\nSplit: a dataset usually have multiple classes and we can group different classes together to form a small dataset. This is the other way.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThey can be all used to construct any of TIL, TIL with task-agnostic testing, CIL. Note that permutations must be the same way to construct one dataset, that’s why we set permutation seeds. Or it ends up with a dataset totally random and can’t be learned anything from."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#metrics-what-cl-cares-about",
    "href": "posts/continual-learning-beginners-guide.html#metrics-what-cl-cares-about",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "3.2 Metrics: What CL Cares About",
    "text": "3.2 Metrics: What CL Cares About\nAlthough the task sequence might be infinite, we have to stop at some point to take a look at how well the model performs. The number of tasks are often set when doing the evaluation on continual learning as we need the comparison under the same standard.\nThere are tests after training each task \\(t\\) on all seen tasks \\(1,\n\\cdots, t\\). After each test on task \\(\\tau\\) using \\(\\mathcal{D}_{\\text{test}}^{(\\tau)}\\), the model can get the accuracy of how many instances are correctly classified. We denote it as \\(a_{\\tau, t}\\) and it forms an upper triangular matrix of metrics. It plays an important role in evaluating continual learning and all the metrics are calculated from it (and some other auxillaries).\n\\[\n\\begin{array}{cccc}\na_{1,1} &  &  & \\cdots \\\\\na_{2,1}  & a_{2,2} &  & \\cdots \\\\\na_{3,1} &  a_{3,2} & a_{3,3} & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\n\\end{array}\n\\]\nAs we said in the definition, the top-one objective of continual learning is to make the model perform well on all tasks. The average accuracy (AA) well represents it as the main performance metric of continual learning. All effort are centered around improving it.\n\\[\\mathrm{AA}_t=\\frac{1}{t} \\sum_{\\tau=1}^t a_{t,\\tau}\\]\nForgetting is a major problem for CL algorithms to address. It manifests as the drop in performance on previously learned tasks. If we sum up all the performance between current accuracy and the accuracy that they drops across previous tasks, we get a metric called backward transfer (BWT). It also reflects the stability of the model because small drop in performance on previous tasks mean keep stable from being significantly changed by training new tasks.\n\\[\n\\mathrm{BWT}_t=\\frac{1}{t-1} \\sum_{\\tau=1}^{t-1}\\left(a_{t,\\tau}-a_{\\tau, \\tau}\\right)\n\\]\nContinual learning algorithms influence the training of each task by encouraging interaction with previously learned tasks. They could have achieved better performance without considering preventing forgetting on previous tasks. The model trained by the task alone is called reference model. If we sum up the difference between continual learning model and this reference model, we get forward transfer (FWT). It reflects the plasticity of the continual learning model, as it measures how closely the model’s performance approaches that of the reference model, which is theoretically most plausible.\n\\[\n\\mathrm{FWT}_t=\\frac{1}{t-1} \\sum_{\\tau=2}^t\\left(a_{\\tau, \\tau}-a^I_\\tau\\right)\n\\]\nAA, BWT, FWT are the most common overall metrics in continual learning. Please check out another post “Understanding Metrics in Continual Learning” which discusses full details of all metrics used in continual learning."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#finetuning-and-fix-the-baselines",
    "href": "posts/continual-learning-beginners-guide.html#finetuning-and-fix-the-baselines",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "3.3 Finetuning and Fix: the Baselines",
    "text": "3.3 Finetuning and Fix: the Baselines\nThe most naive way to finish a continual learning process is to take no action and let the model learn from the sequential data from tasks. At the beginning of each task, it simply initialise the training from the model learned at the end of the last task. That is what we called the Finetuning baseline or sometimes SGD. It is technically not a continual learning algorithm but early CL work usually took it as a baseline. Note that new output heads are sequentially introduced and Finetuning simply initialise them in the way it was supposed to be.\nOn the experiment of Permuted MNIST 10 tasks, ?@fig-Finetuning-results it shows that the last task performance drastically drops every new task arrives, and keep dropping with more new tasks arrive. This leads to a significant drop on the average performance AA.\n\n\n\n\n\n\n\n\n\nWe can tell the direct cause of the poor performance is the unconstraint training on top of previous task, causing a lot of forgetting. If we turn the other way round, we get the Fix approach. This approach simply fixes the model after the training the first task, stops learning aiming to fully address the forgetting. But this lead to the other extreme. On the same experiment, ?@fig-Finetuning-results shows the performance of new task are way worse than the first, also leads to drop on AA as more tasks come to be calculated into average.\nIt is even worse that accuracy goes down to zero in CIL setting for lack of negative examples. That is because the model is convinced to predict only the classes in the new task if only those classes are trained without negative examples for previous tasks. As we know negative examples are very important to machine learning and always suffer from unbalanced data. This shows again that CIL is harder than TIL."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#challenge-1-catastrophic-forgetting",
    "href": "posts/continual-learning-beginners-guide.html#challenge-1-catastrophic-forgetting",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "3.4 Challenge 1: Catastrophic Forgetting",
    "text": "3.4 Challenge 1: Catastrophic Forgetting\nDue to the inherent characteristics of DNN, previous information can hardly be preserved through initialisation after model’s convergence to new tasks. Finetuning suffers a lot from catastrophic forgetting which causes the drastic drop in average performance. Catastrophic Forgetting is the problem in continual learning that learning new tasks can interfere with and degrade performance on previously learned ones. It happens so easily as Finetuning is the most natural way for continual learning and algorithms usually start from here."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#challenge-2-stability-plasticity-dilemma",
    "href": "posts/continual-learning-beginners-guide.html#challenge-2-stability-plasticity-dilemma",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "3.5 Challenge 2: Stability-Plasticity Dilemma",
    "text": "3.5 Challenge 2: Stability-Plasticity Dilemma\nCatastrophic forgetting reflects a low stability feature of the continual learning model. In this research area, most people take it as the main problem and try to promote as much stability as they can. Hundreds of papers talk about continual learning together with catastrophic forgetting.\nHowever, this is not everything. Fix, promotes too much stability then completely lost plasticity. We find two extremes, .ctastrophic forgetting is only one end. As we see in the baseline results, And tilting the balance to the other end is equally problematic. Both two extremes lead to the drastic drop in performance. which tells stability and plasticity contribute vitally in the average performance.\nHowever, A model can’t achieve both stab. Higher stability / plasticity often leads to lower plasticity / stability. it’s like a balance. That‘s Stability-Plasticity Dilemma in deep learning, and it’s particular in continual learning.\nTherefore, a good continual learning algorithm must seek to find some point in between the two extremes, which could balance or trade off between stability and plasticity of the model. That is the way to achieve the best average performance. Continual learning is about the balance of stability and plasticity, not only addressing forgetting.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\nFigure 3: The circle is loss function.\n\n\n\nWe could also think in this way about why performance. We still look at baselines. they both pour all the effort to achieve the better performance of only one task. Finetuning favourites the last task while Fix keep the first task. But considering the long task sequence in continual learning, it is very unvise to give up all other tasks as the objective takes every task into account. @fig- illustrates in a parameter space, and we would rather think a balanced CL algorithm as the best choice."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#challenge-3-network-capacity",
    "href": "posts/continual-learning-beginners-guide.html#challenge-3-network-capacity",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "3.6 Challenge 3: Network Capacity",
    "text": "3.6 Challenge 3: Network Capacity\nLearning AI models is a process of resource distribution. In neural networks, the knowledge from data is distributed to AI models and consolidated into their representation space. In standard machine learning practice, the network size is typically selected according to the data size to avoid underfitting or overfitting issues. If we think like that way in continual learning, we find that the knowledge keeps arriving task after task and never comes to an end. At the assumption of infinite task sequence, we cannot select a proper-sized network beforehand because we never know the data size, and any fixed model will eventually get full and lead to the performance drop. That’s the problem of network capacity.\nSome continual learning algorithms adhere to the assumption under a fixed network, but some do not. They typically expand the network and introduce new parameters, in a linear or dynamic, adaptive way when new task comes up or it reaches the capacity limit.\nIt is not fair to compare algorithms assuming a fixed network with those allowing expanding. Imagine that in TIL, a naive strategy that each new task initialise a model completely independent from previous tasks (it can be called independent learning). It can easily achieve the best performance the same as the reference model mentioned above, which is commonly used as the theoretical upper bound of accuracy in many papers. That’s why some work urges new performance metrics taking the model memory into account to offer a fair evaluation standard."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#replay-based-approaches",
    "href": "posts/continual-learning-beginners-guide.html#replay-based-approaches",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "4.1 Replay-based Approaches",
    "text": "4.1 Replay-based Approaches\nThe most direct way is the get the previous data, and re-train. However, it is the key assumption of CL that no access to previous data. But this assumption is mainly due to a big data memory issue, but we are still allowed with leveraging a small amount of previous data. That is the idea of replay-based approaches. replay-based approaches Prevent forgetting by storing parts of the data from previous tasks▶Replay algorithms use them to consolidate previous knowledge. It is all designed for mimicing the previous task distribution without accessing the whole data.\nThe small amount is not enough to re-train. It has to be representative and operate in a different way other than mix them into training batch with new data. One of the earlist work is iCaRL. It selects the representative samples by some manual algorithms and use them as the reference of knowledge distillation for traininng new tasks. In this work, the memory is fixed and average for every previous tasks, so Some represenattives has to drop when a previous task newly arerives.\nSome approaches generates the previous data from a generator though."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#regularisation-based-approaches",
    "href": "posts/continual-learning-beginners-guide.html#regularisation-based-approaches",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "4.2 Regularisation-based Approaches",
    "text": "4.2 Regularisation-based Approaches\nRegularisation-based approaches add regularisation to loss function. The regularisation is meant for preventing forgetting. The regularisation term is a term in loss function that is also a function of parameters which add up to the normal classification loss of new task \\(\\mathcal{L}^{(t)}_{\\text{cls}}(\\theta) = \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t)}_{\\text{train}}} l(f(\\mathbf{x}; \\theta), y)\\).\n\\[ \\min_\\theta \\mathcal{L}^{(t)}(\\theta) = \\mathcal{L}^{(t)}_{\\text{cls}}(\\theta) + \\lambda R(\\theta) \\]\n\\(\\lambda\\) is the regularisation parameter which works as a hyperparameter controlling the intensity of preventing forgetting, or the scale to balance stability- plasticity trade-off.\nSome will directly regularise on the values of parameters, we call weight regularisation. The most naive way is a regularisation term forcing the parameters for the new tasks to be close to the previous task params:\n\\[ R(\\theta) =  \\sum_{i} \\left(\\theta_i - \\theta_i^{(t-1)}\\right)^2 =  \\|\\theta - \\theta^{(t-1)}\\|^2 \\]\nIt is too coarse that they treat all params equally.\nSome work try to differentiate parameters, and make it regularization different by some importance values:\n\\[ R(\\theta) =   \\sum_{i} \\omega_i \\left(\\theta_i - \\theta_i^{(t-1)}\\right)^2 \\]\nFor example, EWC use the fisher information as importance values, they are calculated after training task t-1 and before training t in the following formula.\n\\[\\omega_i = F_i  =\\frac{1}{N_t} \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t-1)}_{\\text{train}}} \\left[\\frac{\\partial l(f^{(t-1)}\\left(\\mathbf{x}, \\theta), y\\right)}{\\partial \\theta_i}\\right]^2\\]\nThis formula is a result after a series of statistics formula derivation which i don’t wanna go through here, I would rather explain what it means here. For \\(\\theta_i\\), it is the gradient of the loss function to this parameter within the model of after training t-1, showing the sensitivity of that param to performance. If sensitive, that means it is important. It is averaged over the training data which is assumed as the representation of the data distribution of the task t-1.\nEWC is probably the most widely used and successful approach in continual learning.\nSome other works regularise the parameters not in a direct way but rather aim at other components that is a function of parameters. For examples, features, we call feature regularisation. The most naive way is to force the features extracted by the new model to be close to those extract by the previous model, that’s the idea of LwF:\n\\[R_{\\text{LWF}}(\\theta) = \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t)}_{\\text{train}}} l(f(\\mathbf{x};\\theta),f(\\mathbf{x};\\theta^{(t-1)})) \\]\nThis is not very effective and straightforward, but it is simple and works better so rather popular in early continual learning studis."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#architecture-based-approaches",
    "href": "posts/continual-learning-beginners-guide.html#architecture-based-approaches",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "4.3 Architecture-based Approaches",
    "text": "4.3 Architecture-based Approaches\nWhile the replay and regularisation-based approaches try to escape catastrophic forgetting on top of Finetuning by their forgetting preventing mechanisms, architecture-based approaches adopt distinctly different logic of strategy. These approaches leverages the separability characteristic of the neural network architecture. In other words, they treat the network as decomposable resources for tasks, rather than as a whole. The main idea is to dedicate different parts of a neural network to different tasks to minimize the inter-task interference.\nThe “part” of a network can be regarded in various ways, which leads to different kinds of architecture-based approaches.\n\nModular Networks: play around network modules like layers, blocks\n\nParameter Allocation: allocate group of parameters or neurons to task as asubnet Parameter Allocation: OverviewParameter Allocation▶Refines the level of modules to parameters or neurons▶Selects a collection of parameters or neurons to allocate to each task▶Also forms a subnet for the task11/24 Parameter Allocation: Overview▶Weight masks are way greaterthan feature masks in scale▶Should keep a decent amountof neurons in each layerParameter Allocationmethods differ in ways:▶Methods to allocate▶Manually set through hyperparameters▶Learned together with the learning process▶Application of masks during training▶Forward pass▶Backward pass▶Parameter update step▶Application of masks during testing▶Most methods fix the selected subnet aftertrained on their belonged task and use it asthe only model to predict for that task duringtesting\nModel Decomposition: decompose network from various aspects into sharedand task-specific components7/24. The compoenet could be any part of the machine leanring systems like modules itself, or a mathematical decomopositrion of each parameters, in terms of what idea you can come up with .\n\n\nThe challenges\nNetwork Capacity Problem Any fixed model will eventually get full and lead to the performance drop, giventhe potentially infinite task sequence▶Become explicit in architecture-based approaches▶Can be solved by taking shortcuts to expand the networks, but it is not fairStability-Plasticity Trade-Off▶Continual learning seeks to trade off the balance between stability and plasticity▶Approaches that fix part of model for previous tasks are lack of plasticity bystressing too much stability▶Others whichever has task shared components still face the classic catastrophicforgetting problem, which is a result of lack of stability▶They both lead to a bad average performance23/24\n\n\n\n\n\n\nWarning\n\n\n\nNote that architecture-based approaches does not naturally fit in CIL where the tasks are mixed together during test.\n\n\ncheck out"
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#optimization-based-approaches",
    "href": "posts/continual-learning-beginners-guide.html#optimization-based-approaches",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "4.4 Optimization-based Approaches",
    "text": "4.4 Optimization-based Approaches\nThe Explicitly design and manipulate the optimization step. This often involves direct modification of the gradients from \\(g\\) calculated with ordinary loss to \\(g'\\) then use \\(g'\\) for the gradient descent step.\nOne reasonable way project the gradient \\(g\\) to the direction \\(g'\\) not to interfere previous tasks. Many works define this non interference of gradient directions as orthogonal to the directions that could affect previous tasks if updating in that direction.\n\n\n\n\n\n\nTip\n\n\n\nIf you know linear algebra before, the orthogonal of given vectors is very easy calculated by the Gram- Schmidt formula.\nYou might wonder do we have enough space to accommodate many gradients orthogonally. Because We know that a neural network is part of a high dimensional parameter space (larger than or comparable to the number of data points), so there always exist a direction that conforms to the orthogonality condition.\n\n\nOGD preserve a key gradient for each previous tasks and when training new task project the gradient orthogonal to them. The key gradient for previous task should be ideally the current model with respect to the previous data, but since we are done training the previous task we don’t no access to previous data. OGD use some empirical ways to approximate this like save the average gradients that showed up in the steps during training previous task."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#representation-based-approaches",
    "href": "posts/continual-learning-beginners-guide.html#representation-based-approaches",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "4.5 Representation-based Approaches",
    "text": "4.5 Representation-based Approaches\nIn the survey, the author concludes all the approaches that plays around and leveraging the strengths of representation as representation-based approaches. However, I found these methods are new and varys a lot. I would take them as very different approaches. These methods are very new as they adopt the new tech like self-supervised learning, pre-train models.\n\nContinual Learning through Self-Supervised Learning\nSome works try to design special architectures and use some representation learning techinques to learn their own meaningful represenattions and put them into preventing forgetting use. Those techniques are typically involving self-supervised learning like constractive learning.\nThe self-supervised learning has many potential advantages. First, itself can learn a very ：对比学习能够帮助模型学习到更具泛化能力的表示, which is the need for continual learninghelps prevent forgetting. For example, DualNet want the idea of dividing the network fast and slow which corresponds to task- specific and shared . They use Barlow Twins loss for the slow network who want to promote more generalising.\nSecond, many SSL are very useful tools to promote certain goal in the representation . For exmaple, contrastive loss can force promote similarity represention between the sample that are taken as similar, and diffence in represention between the sample that are taken as contrastive, .. If we contrast the new task and previous task , that makes them to be seperate in representations space which is exactly what we want to prevent forgetting. That is how Co2L (Contrastive Continual Learning), the initial work to combine contrastive and continaul learnig does/\n\n\nPre-train Models and Continual Learning\nPretrained large models become very popular since 2018, from BERT to GPT, we all know what it brings to AI research and even our daily life. Continual learning never lost its follow to these hottest advances.\nSome works leverages pre-trained models as a common knowledge for all tasks in continual learning, very straight-forward. Pre-train models create difinetly powerful representtaions so all we have to do is starting from that representation and see what we can get from finetuning to each task. The idea of pre-training is very fit in continual learning.\n随着模型规模的不断增加，全面微调变得计算成本高昂, which makes continual learning either. Prompt-based Learning are proposed around, 。约在2020年，and sooner got popular. 在这种方法中，研究者不是微调整个模型，而是设计特定的提示（prompt），这些提示能够“引导”模型使用其预训练过的知识来解决特定任务, It don’t have to finetuning, without update the network, which is the main casue for catastrophe forgetting。To apply in continual leaning, we have to figure out how to get the prompt for the task from the data. The common way now selecting the most relevant prompts from a pool. For example, the leading work L2P (Learning to Prompt for Continual Learning) have a instance-wise query mechanism to retrieve the proper prompt for that instance. In this instance-wise way, it is decided from the instance itself so we don’t even have to know the task ID which makes it fit task-agnostic testing.\nSome try to make the pre-training itself continuous, that’s CPT (contineal pretraining). They believes the above doesn’t solve the fundamental problem as pre-training model itself is typically collected in an incremental manner and need continual learning. They also believe performing upstream continual learning to improve downstream performance is particularly important from the realistic perspective."
  },
  {
    "objectID": "posts/continual-learning-beginners-guide.html#footnotes",
    "href": "posts/continual-learning-beginners-guide.html#footnotes",
    "title": "A Beginner’s Guide to Continual Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLesort, Timothée, et al. “Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges.” Information fusion 58 (2020): 52-68.↩︎\nVerwimp, Eli, et al. “Clad: A realistic continual learning benchmark for autonomous driving.” Neural Networks 161 (2023): 659-669.↩︎\nShaheen, Khadija, et al. “Continual learning for real-world autonomous systems: Algorithms, challenges and frameworks.” Journal of Intelligent & Robotic Systems 105.1 (2022): 9.↩︎\nHemati, Hamed, Marco Schreyer, and Damian Borth. “Continual learning for unsupervised anomaly detection in continuous auditing of financial accounting data.” arXiv preprint arXiv:2112.13215 (2021).↩︎\nLiu, Shu, et al. “Continual portfolio selection in dynamic environments via incremental reinforcement learning.” International Journal of Machine Learning and Cybernetics 14.1 (2023): 269-279.↩︎\nSingh, Tinku, et al. “An efficient real-time stock prediction exploiting incremental learning and deep learning.” Evolving Systems 14.6 (2023): 919-937.↩︎"
  },
  {
    "objectID": "topics/songbook/index.html",
    "href": "topics/songbook/index.html",
    "title": "Shawn’s Songbook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Artist\n        \n         \n          Album\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nTitle\n\n\nArtist\n\n\nAlbum\n\n\nYear\n\n\n\n\n\n\n\n\n\n每一天我需要祢\n\n\n讚美之泉 (Stream of Praise)\n\n\n我要看見 (I Want to See)\n\n\n2016\n\n\n\n\n\n\n\n我們愛 (讓世界不一樣)\n\n\n讚美之泉 (Stream of Praise)\n\n\n沙漠中的讚美 (Praise in The Desert)\n\n\n2008\n\n\n\n\n\n\n\n滿有能力\n\n\n讚美之泉 (Stream of Praise)\n\n\n不要放棄, 滿有能力 (Do Not Give Up, I Am Strengthened in Him)\n\n\n2009\n\n\n\n\n\n\n\n愛中相遇\n\n\n讚美之泉 (Stream of Praise)\n\n\n這裡有榮耀 (Glory)\n\n\n2014\n\n\n\n\n\n\n\n祢是配得 (聖哉聖哉全地唱)\n\n\n讚美之泉 (Stream of Praise)\n\n\n相信有愛就有奇蹟 (Believe in Love, You Will See Miracles)\n\n\n2011\n\n\n\n\n\n\n\nChrist Is Enough\n\n\nHillsong Worship\n\n\nGlorious Ruins\n\n\n2013\n\n\n\n\n\n\n\n恩典的記號\n\n\n盛曉玫 (Amy Sand)\n\n\n幸福 (Blessed)\n\n\n2012\n\n\n\n\n\n\n\n唯有耶穌\n\n\n讚美之泉 (Stream of Praise)\n\n\n相信有愛就有奇蹟 (Believe in Love, You Will See Miracles)\n\n\n2011\n\n\n\n\n\n\n\n從早晨到夜晚\n\n\n讚美之泉 (Stream of Praise)\n\n\n從早晨到夜晚 (Morning to Night)\n\n\n2017\n\n\n\n\n\n\n\n耶和華祢是我的神\n\n\n讚美之泉 (Stream of Praise)\n\n\n全新的你 (A New You)\n\n\n1999\n\n\n\n\n\n\n\nAmazing Grace\n\n\n \n\n\n \n\n\n1779\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/Jehovah-You-Are-My-God.html",
    "href": "topics/songbook/songs/Jehovah-You-Are-My-God.html",
    "title": "耶和華祢是我的神",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V1-V2 | Interlude-V1-V2-V2-V2 (repeat last)-Outro\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/Jesus-You-Can.html",
    "href": "topics/songbook/songs/Jesus-You-Can.html",
    "title": "唯有耶穌",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V-C | Interlude V-C-C | B | C-C\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/Christ-Is-Enough.html",
    "href": "topics/songbook/songs/Christ-Is-Enough.html",
    "title": "Christ Is Enough",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V1-C-V2-C|Interlude-B1-B2-C-C-B1-B1\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/See-You-Face-to-Face.html",
    "href": "topics/songbook/songs/See-You-Face-to-Face.html",
    "title": "愛中相遇",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure:\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/We-Will-Love-We-Can-Make-a-Difference.html",
    "href": "topics/songbook/songs/We-Will-Love-We-Can-Make-a-Difference.html",
    "title": "我們愛 (讓世界不一樣)",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V-C | I-V-V-C | (+3)C-C-C-C\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html",
    "href": "topics/English-learning/expression-by-categories/kitchen.html",
    "title": "Kitchen",
    "section": "",
    "text": "Frequently used in recipes."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#flake",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#flake",
    "title": "Kitchen",
    "section": "flake",
    "text": "flake\n\nflake\n\n小薄片\n\n\n\nsnow flake (雪花)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#zest",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#zest",
    "title": "Kitchen",
    "section": "zest",
    "text": "zest\n\nzest\n\n磨碎的柠檬皮\n\n\n\nlemon zest"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#cut",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#cut",
    "title": "Kitchen",
    "section": "cut",
    "text": "cut\n\ncut\n\n通用的用刀切的动作, 不指定任何形状.\n\n\n\nBe careful not to cut your hand!"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#chop",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#chop",
    "title": "Kitchen",
    "section": "chop",
    "text": "chop\n\nchop\n\n切成块.\n\n\n\nchopped tomatoes"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#slice",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#slice",
    "title": "Kitchen",
    "section": "slice",
    "text": "slice\n\nslice\n\n切成片. Slice 原意就是片, 这里用作动词.\n\n\n\nsliced bread"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#dice",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#dice",
    "title": "Kitchen",
    "section": "dice",
    "text": "dice\n\ndice\n\n切成丁. Dice 原意是骰子, 这里引申为动词, 可以理解为切成骰子状.\n\n\n\ndiced pancetta"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#mince",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#mince",
    "title": "Kitchen",
    "section": "mince",
    "text": "mince\n\nmince\n\n切成末.\n\n\n\nbeef mince"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#grind",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#grind",
    "title": "Kitchen",
    "section": "grind",
    "text": "grind\n\ngrind\n\n磨成粉\n\n\n\ncoffee grinder (咖啡豆研磨机)\nground pepper\n\n\ngrind\n\n绞肉\n\n\n\nmeat grinder (绞肉机)\nground beef"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crumble",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crumble",
    "title": "Kitchen",
    "section": "crumble",
    "text": "crumble\n\ncrumble\n\n弄碎, 形容饼干, 蛋糕, 墙等.\n\n\n\nbiscuit crumbles (碎饼干)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#stuff",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#stuff",
    "title": "Kitchen",
    "section": "stuff",
    "text": "stuff\n\nstuff\n\n填"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#squeeze",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#squeeze",
    "title": "Kitchen",
    "section": "squeeze",
    "text": "squeeze\n\nsqueeze\n\n挤 d r"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#compress",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#compress",
    "title": "Kitchen",
    "section": "compress",
    "text": "compress\n\ncompress\n\n压缩\n\n\n\ncompressed biscuits (压缩饼干)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#juice",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#juice",
    "title": "Kitchen",
    "section": "juice",
    "text": "juice\n\njuice\n\n动词, 榨汁\n\n\n\njuice a lemon"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#press",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#press",
    "title": "Kitchen",
    "section": "press",
    "text": "press\n\npress\n\n榨\n\n\n\npressed coconut water"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#chill",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#chill",
    "title": "Kitchen",
    "section": "chill",
    "text": "chill\n\nchill\n\n冷却\n\nchilled\n\n冷藏的, 冷冻的是 frozen"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#skim",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#skim",
    "title": "Kitchen",
    "section": "skim",
    "text": "skim"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#drain",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#drain",
    "title": "Kitchen",
    "section": "drain",
    "text": "drain\n\ndrain\n\n沥水\n\n\n\ndrain off the water"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#encase",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#encase",
    "title": "Kitchen",
    "section": "encase",
    "text": "encase\n\nencase\n\n包住\n\n\n\n(Pasty) encased in light puff pastry."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crimp",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crimp",
    "title": "Kitchen",
    "section": "crimp",
    "text": "crimp\n\ncrimp\n\n压出褶皱\n\n\n\nWhen making dumplings, it’s important to crimp the edges tightly to prevent the filling from leaking out during cooking."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crust",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crust",
    "title": "Kitchen",
    "section": "crust",
    "text": "crust\n\ncrust\n\n硬壳.\n\n\n\nAfter baking, the bread developed a golden-brown crust.\npizza crust (披萨边)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crumb",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crumb",
    "title": "Kitchen",
    "section": "crumb",
    "text": "crumb\n\ncrumb\n\n碎屑, 渣\n\n\n\nbreadcrumb (面包糠)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#coating",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#coating",
    "title": "Kitchen",
    "section": "coating",
    "text": "coating\n\ncoating\n\n厚厚一层\n\n\n\na thick coating of chocolate"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#smell",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#smell",
    "title": "Kitchen",
    "section": "smell",
    "text": "smell\n\nsmell\n\n通用的气味, 中性词"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#aroma",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#aroma",
    "title": "Kitchen",
    "section": "aroma",
    "text": "aroma\n\naroma\n\n食物的香味, 褒义"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#odour",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#odour",
    "title": "Kitchen",
    "section": "odour",
    "text": "odour\n\nodour\n\n气味. 书面用语, 通常描述化学, 倾向于贬义"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#scent",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#scent",
    "title": "Kitchen",
    "section": "scent",
    "text": "scent\n\nscent\n\n细腻的香气, 褒义. 形容食物时不适合浓烈的味道."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#flavour",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#flavour",
    "title": "Kitchen",
    "section": "flavour",
    "text": "flavour\n\nflavour\n\n味道\n\nflavoured\n\n…味的\n\n\n\nvanilla flavoured ice cream"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#piping-hot",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#piping-hot",
    "title": "Kitchen",
    "section": "piping hot",
    "text": "piping hot\n\npiping hot\n\n非常热, 热气腾腾. 来源于中世纪，在那个时期，食品用一种名叫“pipe”的器具加热至很高的温度，出锅时通常伴有“嘶嘶”声响，类似于管道中蒸汽的声音，因此形容为“piping hot”\n\n\n\nBe careful, the tea is piping hot and can burn your tongue.\nYour wok needs to be piping hot before stir frying."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#tangy",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#tangy",
    "title": "Kitchen",
    "section": "tangy",
    "text": "tangy\n\ntangy\n\n形容词, 描述强烈而令人愉悦的酸味\n\n\n\ntangy sweetclems\ntangy cheese"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#milky",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#milky",
    "title": "Kitchen",
    "section": "milky",
    "text": "milky\n\nmilky\n\n形容词, 含奶的"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#fluffy",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#fluffy",
    "title": "Kitchen",
    "section": "fluffy",
    "text": "fluffy\n\nfluffy\n\n蓬松的\n\n\n\nfluffy cake"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#succulent",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#succulent",
    "title": "Kitchen",
    "section": "succulent",
    "text": "succulent\n\nsucculent\n\n多汁的"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crisp-crispy",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crisp-crispy",
    "title": "Kitchen",
    "section": "crisp / crispy",
    "text": "crisp / crispy\n\ncrisp / crispy\n\n脆的. crisp 强调食物天然的脆, crispy 强调食物经过煎炸烤等形成的脆\n\n\n\ncrisp apple\ncrispy bacon"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#peppery",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#peppery",
    "title": "Kitchen",
    "section": "peppery",
    "text": "peppery\n\npeppery\n\n辛辣的, 用于形容胡椒, 芝麻菜, 芥末等."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#burnt-burned",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#burnt-burned",
    "title": "Kitchen",
    "section": "burnt / burned",
    "text": "burnt / burned\n\nburnt / burned\n\n糊的, 烧焦的\n\n\n\nThe stir-fry got burnt because I forgot to stir it."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#spicy",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#spicy",
    "title": "Kitchen",
    "section": "spicy",
    "text": "spicy\n\nspicy\n\n辣的, 用于形容辣椒"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#tender",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#tender",
    "title": "Kitchen",
    "section": "tender",
    "text": "tender\n\ntender\n\n嫩的, 用于形容肉或蔬菜\n\n\n\nTenderstem Broccoli (嫩茎西兰花)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#refreshing",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#refreshing",
    "title": "Kitchen",
    "section": "refreshing",
    "text": "refreshing\n\nrefreshing\n\n使人凉爽的;消除疲劳的;提神的\n\n\n\nrefreshing cold beer"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#wholesome",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#wholesome",
    "title": "Kitchen",
    "section": "wholesome",
    "text": "wholesome\n\nwholesome\n\n有益健康的"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#cocoa",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#cocoa",
    "title": "Kitchen",
    "section": "cocoa",
    "text": "cocoa\n\ncocoa /ˈkəʊkəʊ/\n\n可可"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#filling-stuffing",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#filling-stuffing",
    "title": "Kitchen",
    "section": "filling / stuffing",
    "text": "filling / stuffing\n\nfilling / stuffing\n\n馅. stuffing 特指塞在肉或蔬菜里的馅."
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#garnish",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#garnish",
    "title": "Kitchen",
    "section": "garnish",
    "text": "garnish\n\ngarnish\n\n点缀"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#bran",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#bran",
    "title": "Kitchen",
    "section": "bran",
    "text": "bran\n\nbran\n\n麸"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#gluten",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#gluten",
    "title": "Kitchen",
    "section": "gluten",
    "text": "gluten\n\ngluten\n\n麸质\n\n\n\ngluten free (不含麸质)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#net",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#net",
    "title": "Kitchen",
    "section": "net",
    "text": "net\n\nnet\n\n净\n\n\n\nnet weight (净重)"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#yeast",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#yeast",
    "title": "Kitchen",
    "section": "yeast",
    "text": "yeast\n\nyeast\n\n酵母"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#carton",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#carton",
    "title": "Kitchen",
    "section": "carton",
    "text": "carton\n\ncarton\n\n盒子, 装牛奶或果汁的"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#tub",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#tub",
    "title": "Kitchen",
    "section": "tub",
    "text": "tub\n\ntub\n\n小桶 成黄油冰激凌的"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#splash",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#splash",
    "title": "Kitchen",
    "section": "splash",
    "text": "splash\n\nsplash\n\n溅"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#baton",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#baton",
    "title": "Kitchen",
    "section": "baton",
    "text": "baton"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#bar",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#bar",
    "title": "Kitchen",
    "section": "bar",
    "text": "bar"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#mass",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#mass",
    "title": "Kitchen",
    "section": "mass",
    "text": "mass"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#dissipate",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#dissipate",
    "title": "Kitchen",
    "section": "dissipate",
    "text": "dissipate\nodour"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#desiccate",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#desiccate",
    "title": "Kitchen",
    "section": "desiccate",
    "text": "desiccate\n脱水"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#molasses",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#molasses",
    "title": "Kitchen",
    "section": "molasses",
    "text": "molasses\n糖浆"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#syrup",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#syrup",
    "title": "Kitchen",
    "section": "syrup",
    "text": "syrup"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#fortify",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#fortify",
    "title": "Kitchen",
    "section": "fortify",
    "text": "fortify"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#curl",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#curl",
    "title": "Kitchen",
    "section": "curl",
    "text": "curl"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#grain",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#grain",
    "title": "Kitchen",
    "section": "grain",
    "text": "grain"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crinkle",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crinkle",
    "title": "Kitchen",
    "section": "crinkle",
    "text": "crinkle\ncrinkle crisps"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#punchy",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#punchy",
    "title": "Kitchen",
    "section": "punchy",
    "text": "punchy"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#fragrant",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#fragrant",
    "title": "Kitchen",
    "section": "fragrant",
    "text": "fragrant\nfirming agent"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#as",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#as",
    "title": "Kitchen",
    "section": "as",
    "text": "as\nascorbic acid\n维生素C\ntocopherol 维生素E"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#nutty",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#nutty",
    "title": "Kitchen",
    "section": "nutty",
    "text": "nutty\n## molluscs\n软体动物"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#crustaceans",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#crustaceans",
    "title": "Kitchen",
    "section": "crustaceans",
    "text": "crustaceans"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#infuse",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#infuse",
    "title": "Kitchen",
    "section": "infuse",
    "text": "infuse"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#chunk",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#chunk",
    "title": "Kitchen",
    "section": "chunk",
    "text": "chunk"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#scoop",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#scoop",
    "title": "Kitchen",
    "section": "scoop",
    "text": "scoop"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#lengthway",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#lengthway",
    "title": "Kitchen",
    "section": "lengthway",
    "text": "lengthway"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#tray",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#tray",
    "title": "Kitchen",
    "section": "tray",
    "text": "tray"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#film",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#film",
    "title": "Kitchen",
    "section": "film",
    "text": "film"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#trim",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#trim",
    "title": "Kitchen",
    "section": "trim",
    "text": "trim"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#fiery",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#fiery",
    "title": "Kitchen",
    "section": "fiery",
    "text": "fiery"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#tube",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#tube",
    "title": "Kitchen",
    "section": "tube",
    "text": "tube\nraise\nraise\n发面 ## cap"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#punnet",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#punnet",
    "title": "Kitchen",
    "section": "punnet",
    "text": "punnet\nquench 解渴\nquencher 解渴的饮料 ## slit cut a slit"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#coarse",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#coarse",
    "title": "Kitchen",
    "section": "coarse",
    "text": "coarse"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#can",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#can",
    "title": "Kitchen",
    "section": "can",
    "text": "can"
  },
  {
    "objectID": "topics/English-learning/expression-by-categories/kitchen.html#bottle",
    "href": "topics/English-learning/expression-by-categories/kitchen.html#bottle",
    "title": "Kitchen",
    "section": "bottle",
    "text": "bottle"
  },
  {
    "objectID": "topics/English-learning/index.html",
    "href": "topics/English-learning/index.html",
    "title": "English Learning",
    "section": "",
    "text": "Expressions by Categories\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/snacks/nuts.html",
    "href": "topics/cooking-ideas/ingredients/snacks/nuts.html",
    "title": "Shawn's Blog",
    "section": "",
    "text": "peanut almonds hazelnuts walnuts cashew pecan brazil pistachio maacadamia/Queensland\nlupin\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/staple-food/sliced-bread.html",
    "href": "topics/cooking-ideas/ingredients/staple-food/sliced-bread.html",
    "title": "Sliced Bread",
    "section": "",
    "text": "Sliced bread: sliced from bread loaf.\n\nSimplest Cooking\n\nReady to eat\nToaster: Slide one slice of bread into each toaster slot, set the toasting level, wait till ding sound.\n\n\n\n\n\n\n\nCooking Tips\n\n\n\n\nSliced bread can be toasted directly from freezer.\n\n\n\n\n\nClassic Dishes\n\nButtered Toast: spread butter over toasted\nToast with Jam: spread jam over buttered toast\nSandwiches: check out sandwich guide\nAvocado Toast: spread avocado over toast, salt and pepper, with topping like tomatoes, eggs\nFrench Toast: soak bread in eggs and milk, pan fry\n\n\n\nVarieties\n\nIngredients: malted, oatmeal\nBy thickness (thin, medium, thick, extra thick):\n\n\n\nStorage\n\nBread need to be kept in cool, dark, dry place (bread bin / cupboard) away from strong light. Reseal bag after each use.\nBread is freezable. Once frozen use within 3 months\n\n\n\nPurchase Advice\n\nRowan Hill\nHovis\nWarburtons\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/vegetables/rocket.html",
    "href": "topics/cooking-ideas/ingredients/vegetables/rocket.html",
    "title": "Rocket",
    "section": "",
    "text": "Simplest Cooking\n\nAlready washed and ready to eat\n\n\n\nStorage\n\nSalad leaves need to be kept refrigerated. Open opened use within 2 days.\nIt’s best to be placed in the crisper drawer.\nOnce opened it’s best to put a kitchen paper or towel in the bag to get rid of too much moisture and prevent rotten.\nNot freezable.\nWhen spoiled, they get rotten, black and watery.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/vegetables/mixed-vegetables-roast.html",
    "href": "topics/cooking-ideas/ingredients/vegetables/mixed-vegetables-roast.html",
    "title": "Mixed Vegetables (Roast)",
    "section": "",
    "text": "There are packed mixed vegetables ready to roast which you can find in British supermarkets.\n\nSimplest Cooking\n\nOven: on a roasting tray, drizzle 5ml of oil, 200°C middle shelf, 20-25 mins, turn halfway.\n\n\n\nVarieties\n\nMediterranean style: bell pepper, red onion, courgette, cherry tomato.\n\n\n\nStorage\n\nKeep refrigerated. Once opened use within 1 day.\nNot freezable.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/vegetables/mixed-salad-leaves.html",
    "href": "topics/cooking-ideas/ingredients/vegetables/mixed-salad-leaves.html",
    "title": "Mixed Salad Leaves",
    "section": "",
    "text": "Simplest Cooking\n\nAlready washed and ready to eat\n\n\n\nClassical Dishes\n\nSalad Please refer to salad guide.\n\n\n\nVarieties\nIn the supermarkets, the salad leaves are usually packed and mixed. Here are some examples:\n\nbaby leaf = baby red leaf + baby spinach + baby green leaf + red chard Baby leaf” 是指蔬菜幼苗时期的叶子，通常包括各种类型的嫩叶蔬菜。这些叶子在植物的早期生长阶段采摘，质地柔软，味道清新。常见的 “baby leaf” 蔬菜包括：\n\nBaby Spinach（嫩菠菜）：菠菜的幼叶，味道较成熟叶子更柔和。\nBaby Kale（嫩羽衣甘蓝）：羽衣甘蓝的幼叶，较成熟的羽衣甘蓝更嫩，口感不那么粗糙。\nBaby Lettuce（嫩生菜）：各种生菜品种的幼叶，包括罗马生菜、黄油生菜等。\nArugula（芝麻菜）：也有”baby arugula”的形式，幼叶较少辛辣味。\nBaby Chard（嫩甜菜叶）：瑞士甜菜的幼叶，更嫩，更易于使用在沙拉中。\n\nsalad trio = lollo rosso + lollo biondo + oak leaf lettuce\n\n\n\n\nStorage\n\nSalad leaves need to be kept refrigerated. Open opened use within 2 days.\nIt’s best to be placed in the crisper drawer.\nOnce opened it’s best to put a kitchen paper or towel in the bag to get rid of too much moisture and prevent rotten.\nNot freezable.\nWhen spoiled, they get rotten, black and watery.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/mixed-vegetables-ready.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/mixed-vegetables-ready.html",
    "title": "Mixed Vegetables (Ready)",
    "section": "",
    "text": "Simplest Cooking\n\nMicrowave: 3 mins.\nBoil: 5 mins or until tender. Drain.\nSteam: 5 mins or until tender.\n\n\n\nVarieties\n\nCarrot + Broccoli + baby corn + fine beans\n\n\n\nStorage\n\nKeep refrigerated.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/sandwiches.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/sandwiches.html",
    "title": "Sandwich",
    "section": "",
    "text": "Simplest Cooking\n\nReady to eat\n\n\n\nStorage\nKeep refrigerated.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/meat/bacon-rashers.html",
    "href": "topics/cooking-ideas/ingredients/meat/bacon-rashers.html",
    "title": "Bacon Rashers",
    "section": "",
    "text": "Bacon rashers are thin flat pieces of bacon.\n\nSimplest Cooking\n\nPan fry (most common): a little bit oil, moderate/high heat, 5 mins (8 mins for crispy bacon), turn once\nOven: on a baking paper, split up (or they’ll stick together), 200 °C middle shelf, 15-20 mins (25-30 mins for crispy bacon)\nMicrowave (not recommended): 3 mins\nPan fry with water (trending): Add a little bit water just covering the rashers, high heat, until water is reduced down, keep cooking for a while, turn occasionally (you could add some oil unless you prefer it with good fat after water goes)\n\n\n\n\n\n\n\nCondiment\n\n\n\nNo need to season because it’s salty enough.\n\n\n\n\n\n\n\n\nCooking Tips\n\n\n\n\nControl your favourite texture by cooking time, from tender to crispy;\nIt is easier with small kitchen tongs;\nAdditional fats can be saved for other uses like frying eggs.\n\n\n\n\n\nClassic Dishes\nBacon is traditionally for English breakfast, and also essential to sandwiches.\n\nSandwich:\n\nBacon Sandwich: pure bacon sandwich, with brown sauce / ketchup\nBLT: bacons, lettuce and tomato sandwich, with mayo\nBEC: bacons, eggs and cheese sandwich\n\nFull Breakfast: pan fry bacon, eggs, sausages, mushrooms, tomatoes, baked beans etc with toasts\nPasta:\n\nCreamy Pasta: bacons or chickens, cream and mushroom pasta\n\nPigs in Blankets: wrap sausage with bacon rashers, oven cook\n\n\n\nInnovative Dishes\n\nBacon can be used as a substitute of salted pork in any Chinese dishes:\n\nStir-fry Tenderstem with Bacon\n\n\n\n\nVarieties\n\nBy thickness: thin cut / standard / thick cut\n\nIrregular shape bacon chunks are available at some supermarket. We can slice them into rashers no thicker than 1cm.\n\nSmoked/ unsmoked\nRindless or not\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/index.html",
    "href": "topics/cooking-ideas/index.html",
    "title": "Cooking Ideas",
    "section": "",
    "text": "Meat\n\n\n\n\n\n\n\n\n\n\nEggs\n\n\n\n\n\n\n\n\n\n\n\n\n\nPork Sausages (British)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeef Mince\n\n\n\n\n\n\n\n\n\n\n\n\n\nBacon Rashers\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nVegetables\n\n\n\n\n\n\n\n\nTenderstem Broccoli\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Salad Leaves\n\n\n\n\n\n\n\n\n\n\n\n\n\nRocket\n\n\n\n\n\n\n\n\n\n\n\nMixed Vegetables (Stir Fry)\n\n\n\n\n\n\n\n\n\n\n\nMixed Vegetables (Roast)\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nStaple Food\n\n\n\n\n\n\n\n\n\n\nSliced Bread\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nDairy\n\n\n\n\n\nNo matching items\n\n\n\n\nSeafood\n\n\n\n\n\nNo matching items\n\n\n\n\nFruit\n\n\n\n\n\nNo matching items\n\n\n\n\nSpices\n\n\n\n\n\nNo matching items\n\n\n\n\nSeasoning\n\n\n\n\n\nNo matching items\n\n\n\n\nReady Meals\n\n\n\n\n\n\n\n\n\n\nPizza\n\n\n\n\n\n\n\n\n\n\n\n\n\nSandwich\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Vegetables (Ready)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza\n\n\n\n\n\n\n\n\n\n\n\n\n\nChinese Dumplings\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Vegetables (Ready)\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html",
    "href": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html",
    "title": "Shandong Stir-fried Chicken (山东炒鸡)",
    "section": "",
    "text": "Shandong Stir-fried Chicken (山东炒鸡) is a dish originated from Shandong province in China, my hometown."
  },
  {
    "objectID": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#main",
    "href": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#main",
    "title": "Shandong Stir-fried Chicken (山东炒鸡)",
    "section": "Main",
    "text": "Main\nServe 3-4 people\n\nWhole Chicken\n\nAlternatively in British supermarket, 1kg Chicken Drumsticks and 1kg Chicken Thighs\n\nChilli: like the amount of 3-4 peppers\n\nPreferably Zaozhuang Thin-Skinned Chili\n\nGarlic: 2-3 heads\n\nYes we need generous amount of garlic to enhance flavor."
  },
  {
    "objectID": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#seasonings",
    "href": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#seasonings",
    "title": "Shandong Stir-fried Chicken (山东炒鸡)",
    "section": "Seasonings",
    "text": "Seasonings\n\nSpring Onion\nGinger\nAnise\nDried Chilli\nXiaomi Chilli\nSichuan Pepper\nCoriander\nLight Soy Sauce\nChinese Cooking Wine\nYellow Soybean Paste\nSichuan Doubanjiang (optional)\nRice Vinegar\nSesame Oil\nFinishing Oil (optional)\nOil\nSalt\nCaster Sugar (optional)\nMSG (optional)"
  },
  {
    "objectID": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#prepare",
    "href": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#prepare",
    "title": "Shandong Stir-fried Chicken (山东炒鸡)",
    "section": "Prepare",
    "text": "Prepare\n\nWash and chop the whole chicken into large pieces.\nWash and cut the chillies into irregular pieces (can be done later during simmering the chicken)\nSmash the garlic (can be done later during simmering the chicken)\nChop the spring onion into chunks\nSlice the ginger\nChop the coriander (can be done later during simmering the chicken)\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s best to use some force and make each cut in one go to avoid creating bone fragments when chopping chicken."
  },
  {
    "objectID": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#cook",
    "href": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#cook",
    "title": "Shandong Stir-fried Chicken (山东炒鸡)",
    "section": "Cook",
    "text": "Cook\n\nHeat some oil, fry the anise at 40% oil heat, fry the spring onion and ginger till fragrant at 60% oil heat.\n\n\n\n\n\n\n\nTip\n\n\n\nBe accurate about the amount of oil. The chicken releases oil during cooking.\n\n\n\nAdd chicken, keep still to brown a bit, turn once.\n\n\n\n\n\n\n\nTip\n\n\n\nThe chicken releases water which makes it look like half frying half simmering. Wait until the liquid oily not watery. Be patient.\n\n\n\nAdd and stir thoroughly:\n\n\nDried chilli: depend on how spicy you want. You need to know about how spicy your chilli is.\nSichuan pepper: don’t put too much or it will break the original flavor according to my experience.\nChinese cooking wine: 4 tbsp\nSichuan Donbanjiang: 2 tbsp (optional)\nYellow soybean paste: 6 tbsp\nRice vinegar: 2 tbsp\nLight soy sauce: 6 tbsp\n\n\nAdd water and boil. When boiled, remove the scum if any. Simmer. Add Xiaomi chillies, salt (1 tbsp), caster sugar (optional, 1 tbsp), MSG (optional, 1 tbsp) halfway.\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe amount of water determines the simmering time, and it should be adjusted according to the type of chicken. For example, chickens in the UK are more tender and don’t require long cooking times, so don’t add too much water. Cook the chicken until it’s just done; if cooked too long, it will become mushy.\n\nFor fresh chicken in China, add water to just cover.\nFor British chicken, add water to half cover.\n\nRemoving the scum is essential, otherwise, the dish will taste greasy.\n\n\n\n\nTurn to medium heat when you think it’s nearly done. Add chillies and garlic. Reduce the water to a bit soup base for moisturing the dish.\n\n\n\n\n\n\n\nTip\n\n\n\nThis step requires careful timing and heat! For watery varieties of chilli (like peppers), if the heat is too low, more water will be released, and the water will never be reduced. On the other hand, don’t let the water dry out too much. The time of cooking chillies and garlic also needs to be just right. If it’s too long, they will become mushy instead of crunchy; if too short, they will have a raw taste.\n\n\n\nOff the heat, drizzle some fresh light soy sauce, sesame oil, finishing oil (optional), add the coriander."
  },
  {
    "objectID": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#variations",
    "href": "topics/cookbook/recipes/Shandong-stir-fried-chicken.html#variations",
    "title": "Shandong Stir-fried Chicken (山东炒鸡)",
    "section": "Variations",
    "text": "Variations\nYou can add some potato chunks . The medium sized potato chunks need 15-20 mins to cook so add them when simmering with around 15-20 mins left.\n\n\n\n\n\n\nTip\n\n\n\nPotato chunks absorb some spices so remember to add a bit more salt or soy sauce."
  },
  {
    "objectID": "topics/cookbook/index.html",
    "href": "topics/cookbook/index.html",
    "title": "Cookbook",
    "section": "",
    "text": "General Guides\n\n\n\n\n\nNo matching items\n\n\n\n\nRecipes\n\n\n\n\n\n\n\n\n\n\nShandong Stir-fried Chicken (山东炒鸡)\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cookbook/general-recipes/poach.html",
    "href": "topics/cookbook/general-recipes/poach.html",
    "title": "Poaching (白灼)",
    "section": "",
    "text": "Poaching (白灼) is a traditional Cantonese cooking method which is similar to boiling. It keeps the dish very light and healthy, and is also very handy.\n\nIngredients\nThe poaching technique is supposed to highlight the freshness and quality of the ingredients. Use the ingredients as fresh as possible.\nThey can be vegetables:\n\nChoy sum\nRomaine lettuce\nBroccoli\nOkra\nAsparagus\n\nSeafood\n\nPrawns\nSliced fish\n\n\n\nPoach\nNext step is poaching the ingredients. Poaching is different from boiling in terms of the time and temperature of the boiling water.\n\n\n\n\n\n\nTip\n\n\n\nAdd some oil and salt to the water for keeping the colour of the vegetables.\n\n\n\n\nSauce\nThe sauce is usually composed of:\n\nSolid stuff: spring onion (shredded), ginger (shredded), garlic (minced), pepper (shredded), bird’s eye chilli (sliced)\nHot Oil\nSoy sauce: could add sugar, oyster sauce, cooking wine to enhance the fresh flavour. Better use the steamed fish soy sauce (蒸鱼豉油). Thin it out a bit with water or the boiled water used for poaching.\n\nPour the hot oil over the solids to stimulate their aroma. Then add the soy sauce. Do not mix the soy sauce first with solids as they won’t be in touch with hot oil and stimulated.\nThe sauce can be served as dipping sauce (often for seafoods), or can be poured over the po ached ingredients (often for vegetables).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cv/cv-zh.html",
    "href": "cv/cv-zh.html",
    "title": "Shawn's Blog",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "cv/academic.html",
    "href": "cv/academic.html",
    "title": "Pengxiang Wang",
    "section": "",
    "text": "Research Insterests\nI am doing AI and machine learning research as a PhD student right now, mainly insterested in continual learning.\n\n\nEducation\n\n2023.12-, University of Bristol\n\nVisiting research associate\nIn School of Engineering Mathematics and Technology (SEMT)\nSupervisor: Prof Weiru Liu, Prof Jun Hong\n\n2020-, Peking University\n\nIn School of Mathematical Sciences (SMS)\nCourse: Applied Mathematics, PhD (candidate)\nSupervisor: Kedian Mu\n\n2016-2020, Beijing Institute of Technology\n\nIn School of Mathematics and Statistics\nCourse: Information and Computing Science, BSc\n\n\n\n\nPublication\n\nAdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning\n\nAccepted by ECML PKDD 2024\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hey guys! This is Shawn’s page, and blog as well. I am just putting everything related to me here, including academic posts, and notes and sharings on any personal interests and hobbies.\nJust a brief introduction of me. I am a PhD student in China and currently based in the UK. You could see all my academic information top-left of this page. Besides that, I have wide interests in something like music, cooking, language learning and stuff, which you could find here beyond those nerdy academic posts!\nFeel free to leave a message on my social media below! :))\n\n\n Back to top"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html",
    "href": "slides/slides-continual-learning-beginners-guide.html",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Machine Learning Paradigms\n\nHow the data distribute and the way that they are allowed to be used\nHow the model is evaluated\nWithout looking into what types of data are\nE.g., \n\nScenario\n\nThe real-world applications\nE.g., \n\nContinual Learning (CL)\n\nA.k.a. Lifelong Learning, Incremental Learning, Sequential Learning\nA machine learning paradigm involving multiple tasks\n\n\n\n\n\nContinual learning is a machine learning paradigm where an algorithm receives the data from tasks sequentially without the access to previous ones to learn a model that performs the best for all tasks.\n\nKey Features:\n\nSequential tasks\n\nNon-stationary data: tasks from different distributions (difficult!)\n\nTest for all tasks (difficult!)\nNo access to previous task’s data\n\nPractical considerations: huge memory cost / potential violence to privacy\nIt’s not joint training. Problems: computation cost, induction bias\n\nInfinite sequence of tasks\n\nNever know the future challenges\n\n\n\n\n\n\n\nStandard Supervised Learning: continual learning with one task\n\n\n\n\n\n\nMulti-Task Learning: tasks sequentially \\(\\rightarrow\\) at the same time\n\n\n\n\n\n\nTransfer Learning / Domain Adaptation: 2 tasks, test for all tasks \\(\\rightarrow\\) for the second task\n\n\n\n\n\n\nOnline Learning: single-task paradigm, data are from same distribution\n\n\n\n\n\n\nMeta Learning: test for all tasks \\(\\rightarrow\\) for unseen new tasks, learn to learn with meta learner\n\n\n\n\nOne of the most important feature in human learning is to learn and adapt new knowledge continuously without forgetting previous knowledge.\n\\[\\Downarrow\\]\n\\[\\Downarrow\\]\nFit in any real world application s facing a continuous stream of non-stationary data when it’s a bad idea to retrain from scratch.\n\n\n\n\nContinual learning is still in its early age without so many examples of applications due to its difficulty, but it is a highly potential solution to any real-world applications.\n\n\nIn Robotics\n\nRobotic agents are naturally playground for continual learning because of they interact with real world, and some might say CL is born for robotics\nVarious scenarios like object detection, segmentation, reinforcement learning which face the non-stationary data challenges.\n\nIn Autonomous Driving\n\nThe environment and driving conditions are constantly changing, like weather, traffic, objects\n\nIn Finance\n\nFor example, anomaly detection in auditing: a company might face different patterns of frauds in their financial quarters or years.\nOther applications: algorithm trading, portfolio selection, financial forecasting, credit scoring.\n\nIn CV, NLP, recommendation systems, health care, etc."
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#the-scope",
    "href": "slides/slides-continual-learning-beginners-guide.html#the-scope",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Machine Learning Paradigms\n\nHow the data distribute and the way that they are allowed to be used\nHow the model is evaluated\nWithout looking into what types of data are\nE.g., \n\nScenario\n\nThe real-world applications\nE.g., \n\nContinual Learning (CL)\n\nA.k.a. Lifelong Learning, Incremental Learning, Sequential Learning\nA machine learning paradigm involving multiple tasks"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#definition-of-continual-learning",
    "href": "slides/slides-continual-learning-beginners-guide.html#definition-of-continual-learning",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Continual learning is a machine learning paradigm where an algorithm receives the data from tasks sequentially without the access to previous ones to learn a model that performs the best for all tasks.\n\nKey Features:\n\nSequential tasks\n\nNon-stationary data: tasks from different distributions (difficult!)\n\nTest for all tasks (difficult!)\nNo access to previous task’s data\n\nPractical considerations: huge memory cost / potential violence to privacy\nIt’s not joint training. Problems: computation cost, induction bias\n\nInfinite sequence of tasks\n\nNever know the future challenges"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms",
    "href": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Standard Supervised Learning: continual learning with one task"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Multi-Task Learning: tasks sequentially \\(\\rightarrow\\) at the same time"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-2",
    "href": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-2",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Transfer Learning / Domain Adaptation: 2 tasks, test for all tasks \\(\\rightarrow\\) for the second task"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-3",
    "href": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-3",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Online Learning: single-task paradigm, data are from same distribution"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-4",
    "href": "slides/slides-continual-learning-beginners-guide.html#differences-from-other-paradigms-4",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Meta Learning: test for all tasks \\(\\rightarrow\\) for unseen new tasks, learn to learn with meta learner"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#why-continual",
    "href": "slides/slides-continual-learning-beginners-guide.html#why-continual",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "One of the most important feature in human learning is to learn and adapt new knowledge continuously without forgetting previous knowledge.\n\\[\\Downarrow\\]\n\\[\\Downarrow\\]\nFit in any real world application s facing a continuous stream of non-stationary data when it’s a bad idea to retrain from scratch."
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#potential-applications",
    "href": "slides/slides-continual-learning-beginners-guide.html#potential-applications",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "",
    "text": "Continual learning is still in its early age without so many examples of applications due to its difficulty, but it is a highly potential solution to any real-world applications.\n\n\nIn Robotics\n\nRobotic agents are naturally playground for continual learning because of they interact with real world, and some might say CL is born for robotics\nVarious scenarios like object detection, segmentation, reinforcement learning which face the non-stationary data challenges.\n\nIn Autonomous Driving\n\nThe environment and driving conditions are constantly changing, like weather, traffic, objects\n\nIn Finance\n\nFor example, anomaly detection in auditing: a company might face different patterns of frauds in their financial quarters or years.\nOther applications: algorithm trading, portfolio selection, financial forecasting, credit scoring.\n\nIn CV, NLP, recommendation systems, health care, etc."
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#formal-definition-of-continual-learning-classification",
    "href": "slides/slides-continual-learning-beginners-guide.html#formal-definition-of-continual-learning-classification",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Formal Definition of Continual Learning Classification",
    "text": "Formal Definition of Continual Learning Classification\nIn continual learning classification problem, we have:\n\nTasks: \\(t=1,2,\\cdots\\)\nTraining data of tasks: \\(\\mathcal{D}_{\\text{train}}^{(t)}=\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^{N_t} \\in (\\mathcal{X}^{(t)},\\mathcal{Y}^{(t)})\\)\nTesting data of tasks as well: \\(\\mathcal{D}_{\\text{test}}^{(t)} \\in (\\mathcal{X}^{(t)},\\mathcal{Y}^{(t)})\\)\n\nWe aim to develop an algorithm which trains the model \\(f^{(t-1)}\\) to \\(f^{(t)}\\) at the time for task \\(t\\):\n\nWith access to \\(\\mathcal{D}_{\\text{train}}^{(t)}\\) only\nTo perform well on all seen tasks \\(\\mathcal{D}_{\\text{test}}^{(1)}, \\cdots, \\mathcal{D}_{\\text{test}}^{(t)}\\)"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#the-multi-head-classifier",
    "href": "slides/slides-continual-learning-beginners-guide.html#the-multi-head-classifier",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "The Multi-head Classifier",
    "text": "The Multi-head Classifier\nCL Model = Backbone Network + Multi-head Classifier\nMulti-head classifier\n\nOutput heads assigned to different tasks\nA head = simply a linear output layer outputing logits of classes\nNew head is initialised and trained along with backbone as new task come in"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#the-multi-head-classifier-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#the-multi-head-classifier-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "The Multi-head Classifier",
    "text": "The Multi-head Classifier"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#til-vs-cil",
    "href": "slides/slides-continual-learning-beginners-guide.html#til-vs-cil",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "TIL vs CIL",
    "text": "TIL vs CIL\nTask-Incremental Learning (TIL)\n\nKnown task ID during testing: \\((\\mathbf{x}, y, t)\\in (\\mathcal{X}^{(t)},\\mathcal{Y}^{(t)})\\)\nOutput heads are segregated, without considering other tasks\nA-level analogy: separately conducted exams, students know which subject is tested on.\n\nClass-Incremental Learning (CIL)\n\nClasses from all tasks to predict from: \\((\\mathbf{x}, y) \\in (\\mathcal{X}^{(1)}\\cup \\cdots\\cup \\mathcal{X}^{(t)},\\mathcal{Y}^{(1)}\\cup \\cdots\\cup \\mathcal{Y}^{(t)})\\), without known task ID\nIncremental evolving output heads\nA-level analogy: one crazily huge exam including and mixing all subjects\nMuch more difficult than TIL!"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#til-vs-cil-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#til-vs-cil-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "TIL vs CIL",
    "text": "TIL vs CIL\nMore paradigms:\n\nTask-agnostic testing: TIL without known test ID. the model has to figure out the test ID by itself\nTask-agnostic continual learning: eliminate the task boundary"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#logistics",
    "href": "slides/slides-continual-learning-beginners-guide.html#logistics",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Logistics",
    "text": "Logistics\n\n2 naive baseline algorithms\nEvaluated on a simple 10-task CL dataset\nAnalyse the results and introduce the challenges faced by CL\n\nBefore that, I give a tour around:\n\nWhere to evaluate the algorithms: the construction of CL datasets\nHow to evaluate the algorithms: the metrics that CL cares about"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#how-to-construct-cl-dataset-permute-split-combine",
    "href": "slides/slides-continual-learning-beginners-guide.html#how-to-construct-cl-dataset-permute-split-combine",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "How to Construct CL Dataset: Permute, Split, Combine",
    "text": "How to Construct CL Dataset: Permute, Split, Combine\nCombine\n\nFrom different sources of ML datasets, each serving as a task\nBe aware varied input dimensions\nE.g., \n\nConstructed from one dataset:\n\nPermute: permute image pixels in the original dataset under a same certain way to get for a task\nSplit: split the original dataset by group of classess to form subsets for different tasks"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#how-to-construct-cl-dataset-permute-split-combine-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#how-to-construct-cl-dataset-permute-split-combine-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "How to Construct CL Dataset: Permute, Split, Combine",
    "text": "How to Construct CL Dataset: Permute, Split, Combine"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#metrics-what-cl-cares-about",
    "href": "slides/slides-continual-learning-beginners-guide.html#metrics-what-cl-cares-about",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Metrics: What CL Cares About",
    "text": "Metrics: What CL Cares About\nThe lower triangular matrix: the main result\n\\[\n\\begin{array}{cccc}\na_{1,1} &  &  & \\cdots \\\\\na_{2,1}  & a_{2,2} &  & \\cdots \\\\\na_{3,1} &  a_{3,2} & a_{3,3} & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\n\\end{array}\n\\]\n\n\\(a_{t,\\tau}\\): the accuracy of \\(f^{(t)}\\) after training task \\(t\\), testing on task \\(\\tau\\) testset \\(\\mathcal{D}_{\\text{test}}^{(\\tau)}\\)\n\nAverage Accuracy (AA)\n\\[\\mathrm{AA}_t=\\frac{1}{t} \\sum_{\\tau=1}^t a_{t,\\tau}\\]\n\nThe main performance metric to make effort to improve"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#metrics-what-cl-cares-about-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#metrics-what-cl-cares-about-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Metrics: What CL Cares About",
    "text": "Metrics: What CL Cares About\nBackward Transfer (BWT) \\[\n\\mathrm{BWT}_t=\\frac{1}{t-1} \\sum_{\\tau=1}^{t-1}\\left(a_{t,\\tau}-a_{\\tau, \\tau}\\right)\n\\]\n\nForgetting measure: summed up drop in performance on previously learned tasks\nStability measure: how stable the model changed after training new tasks\n\nForward Transfer (FWT) \\[\n\\mathrm{FWT}_t=\\frac{1}{t-1} \\sum_{\\tau=2}^t\\left(a_{\\tau, \\tau}-a^I_\\tau\\right)\n\\]\n\n\\(a^I_\\tau\\): the performance of reference model, trained with task \\(\\tau\\) alone\nCL could have achieved better performance as reference model without considering preventing forgetting on previous tasks\nPlasticity measure: summed up difference from reference model – the most plausible model"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#finetuning-and-fix-the-baselines",
    "href": "slides/slides-continual-learning-beginners-guide.html#finetuning-and-fix-the-baselines",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Finetuning and Fix: the Baselines",
    "text": "Finetuning and Fix: the Baselines\nTwo naive baselines for continual learning paradigm:\nFinetuning (or SGD)\n\nSimply initialise from the model learned from last task\nTake no action to prevent forgetting, let it go\n\nFix\n\nFix the model from being updated after training first task\nThe other way around which tries to fully prevent forgetting"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#finetuning-and-fix-the-baselines-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#finetuning-and-fix-the-baselines-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Finetuning and Fix: the Baselines",
    "text": "Finetuning and Fix: the Baselines"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#challenge-1-catastrophic-forgetting",
    "href": "slides/slides-continual-learning-beginners-guide.html#challenge-1-catastrophic-forgetting",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Challenge 1: Catastrophic Forgetting",
    "text": "Challenge 1: Catastrophic Forgetting\nCatastrophic Forgetting\n\nPrevious knowledge can hardly be preserved within neural networks after convergence to a different data distribution\nFinetuning suffers the most\nThe main problem that most CL alogirthms make effort to address\n\n\n\n\n\n\n\nTip\n\n\n\nIn CIL, forgetting is even more catastrophic because of the lack of negative examples, which shows CIL is much more difficult than TIL.\n\n\nHowever, CL is not all about catastrophic forgetting. It is just one side of the coin."
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#challenge-2-stability-plasticity-dilemma",
    "href": "slides/slides-continual-learning-beginners-guide.html#challenge-2-stability-plasticity-dilemma",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Challenge 2: Stability-Plasticity Dilemma",
    "text": "Challenge 2: Stability-Plasticity Dilemma\nThe Problem of the Other Extreme\n\nFully prevent forgetting = promote too much stability \\(\\rightarrow\\) completely lose plasticity\nAs we can see in the extreme effort of Fix, both two extremes lead to bad average performance\n\n\n\n\n\nStability-Plasticity Dilemma\n\nThe model cannot achieve both stability and plasticity at the same time\nCL algorithms have to trade-off the balance of stability-plasticity"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#challenge-3-network-capacity",
    "href": "slides/slides-continual-learning-beginners-guide.html#challenge-3-network-capacity",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Challenge 3: Network Capacity",
    "text": "Challenge 3: Network Capacity\nNetwork Capacity Problem\n\nAny fixed model will eventually get full as infinite tasks arrive\nCannot select a proper-sized network beforehand under the infinte task assumption\n\nThe network: fixed or expanded?\nIndependent Learning\n\nA prohibited way to do continual learning\nFully expanded network capacity, causing linear increasing model memory cost\nAchieve the best performance as the reference models. Not fair!\n\nThe metrics taking into account model memory cost?"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#replay-based-approaches",
    "href": "slides/slides-continual-learning-beginners-guide.html#replay-based-approaches",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Replay-based Approaches",
    "text": "Replay-based Approaches\n\nIn CL definition: no access to the previous data\nMainly due to a data memory issue\nStill allow storing a small amount of previous data\n\nReplay-based Approaches:\n\nStore a small amount of representations of previous data\nTry to mimic the previous task distribution\nLeverage them by replay mechanisms\n\nTwo steps for Replayed Data:\n\nSampling:\n\nManually select by certain importance measure\nGenerated by generative model (pseudo replay)\nSome samples features to store (feature replay)\n\nUtilizing:\n\nReplayed data are not enough to be mixed and trained with new data\nMechanism like knowledge distillation, optimization constraints\n\n\nThe metrics taking into account data memory cost?"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#regularisation-based-approaches",
    "href": "slides/slides-continual-learning-beginners-guide.html#regularisation-based-approaches",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Regularisation-based Approaches",
    "text": "Regularisation-based Approaches\nRegularisation-based Approaches\n\nAdd regularisation for preventing forgetting to loss function:\n\n\\[ \\min_\\theta \\mathcal{L}^{(t)}(\\theta) = \\mathcal{L}^{(t)}_{\\text{cls}}(\\theta) + \\lambda R(\\theta) \\]\n\\[\\mathcal{L}^{(t)}_{\\text{cls}}(\\theta) = \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t)}_{\\text{train}}} l(f(\\mathbf{x}; \\theta), y)\\]\n\nRegularisation parameter \\(\\lambda\\): hyperparameter, controlling the intensity of preventing forgetting, or the scale to balance stability-plasticity trade-off"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#regularisation-based-approaches-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#regularisation-based-approaches-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Regularisation-based Approaches",
    "text": "Regularisation-based Approaches\nWeight Regularisation\n\nThe naive way:\n\n\\[ R(\\theta) =  \\sum_{i} \\left(\\theta_i - \\theta_i^{(t-1)}\\right)^2 =  \\|\\theta - \\theta^{(t-1)}\\|^2 \\]\n\nWith parameter importance:\n\n\\[ R(\\theta) =   \\sum_{i} \\omega_i \\left(\\theta_i - \\theta_i^{(t-1)}\\right)^2 \\]\n\nIn EWC, 2017: \\[\\omega_i = F_i  =\\frac{1}{N_t} \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t-1)}_{\\text{train}}} \\left[\\frac{\\partial l(f^{(t-1)}\\left(\\mathbf{x}, \\theta), y\\right)}{\\partial \\theta_i}\\right]^2\\]"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#regularisation-based-approaches-2",
    "href": "slides/slides-continual-learning-beginners-guide.html#regularisation-based-approaches-2",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Regularisation-based Approaches",
    "text": "Regularisation-based Approaches\nFeature Regularisation\n\nImplicitly regularise the parameters by constraining features\nThe naive way (LwF, 2016):\n\n\\[R_{\\text{LWF}}(\\theta) = \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t)}_{\\text{train}}} l(f(\\mathbf{x};\\theta),f(\\mathbf{x};\\theta^{(t-1)})) \\]"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches",
    "href": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Architecture-based Approaches",
    "text": "Architecture-based Approaches\nArchitecture-based Approaches\n\nA distintly different strategy that decomposes the network\nDedicate different parts of a neural network to different tasks\nMinimize the inter-task interference\nLeverages the separability characteristic of the neural network architecture\n\nHow to define “parts”:\n\nModular Networks: play around network modules like layers, blocks\nParameter Allocation: allocate group of parameters or neurons to task as asubnet\nModel Decomposition: decompose network from various aspects into sharedand task-specific components\n\nChallenges:\n\nNetwork capacity becomes explicit\nTend to fix part of model for previous tasks, stress stability, lack plasticity"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches-1",
    "href": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches-1",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Architecture-based Approaches",
    "text": "Architecture-based Approaches\n\n\n\n\nProgressive Networks, 2016\n\nExpand the network with new column module for each new task\nLinearly increasing model memory\nSimilar to independent training: train a independent network for each task"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches-2",
    "href": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches-2",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Architecture-based Approaches",
    "text": "Architecture-based Approaches\nHAT (Hard Attention to the Task), 2018\n\nMasks and parameters are both learnable\nFix masked parameters once trained until testing using the subnet\nSparsity regularization for masks\n\n\n\n\n\nAdaHAT, 2024 (my work)\n\nAllow minor adaptive adjustment to masked parameters"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches-3",
    "href": "slides/slides-continual-learning-beginners-guide.html#architecture-based-approaches-3",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Architecture-based Approaches",
    "text": "Architecture-based Approaches\nACL (Adversarial Continual Learning), 2020\n\nShared and task-specific, modules, features\nShared module is adversarially trained with the discriminator to generate task-invariant features. The discriminator predicts task labels"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#optimization-based-approaches",
    "href": "slides/slides-continual-learning-beginners-guide.html#optimization-based-approaches",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Optimization-based Approaches",
    "text": "Optimization-based Approaches\nOptimization-based Approaches\n\nExplicitly design and manipulate the optimization step\nOften involves direct modification of the gradients\n\nOrthogonal gradients projection:\n\nProject the gradient \\(g\\) to the direction \\(g'\\) orthogonal to the previous space\nPrevent interfering previous tasks in the gradient descent level\n\nThe orthogonal projection: Gram-Schmidt formulas\n\\[ u_1 = v_1,\\ u_k = v_k - \\sum_{i=1}^{k-1} \\text{proj}{u_i}(v_k),\\ \\text{proj}{u_i}(v_k) = \\frac{v_k \\cdot u_i}{u_i \\cdot u_i} u_i\n\\]"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#continual-learning-self-supervised-learning",
    "href": "slides/slides-continual-learning-beginners-guide.html#continual-learning-self-supervised-learning",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Continual Learning + Self-Supervised Learning",
    "text": "Continual Learning + Self-Supervised Learning\nSelf-Supervised Learning (SSL)\n\nCan help models learn more generalized representations, essential for continual learning to prevent forgetting\nE.g.\n\n\n\n\n\nContrastive Learning\n\nContrastive loss encourages similar representations for samples considered similar, distinct representations for samples regarded as contrasting\nE.g."
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#continual-learning-pre-trained-models",
    "href": "slides/slides-continual-learning-beginners-guide.html#continual-learning-pre-trained-models",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Continual Learning + Pre-trained Models",
    "text": "Continual Learning + Pre-trained Models\nFinetuning for downstream continual learning\n\nBecome popular along with per-trained models like Transformer, BERT\nShared = pre-trained model, task-specific = finetuning for each task in CL\n\nPrompt-based continual learning\n\nPrompt doesn’t need updating parameters, solves the problem of cost to finetune\nBecome popular along with larger pre-trained models like GPT\nShared = pre-trained model, task-specific = prompts for each task in CL\nE.g.\n\nSelect the most relevant prompts from a pool}\nInstance-wise query mechanism to retrieve prompt, task-agnostic}\n\n\nContinual Pre-Training (CPT)\n\nSolve the continual learning problem of pre-training model itself"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#other-trends",
    "href": "slides/slides-continual-learning-beginners-guide.html#other-trends",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Other Trends",
    "text": "Other Trends\nExtended Paradigms:\n\nFew-Shot Continual Learning (FSCL)\nUnsupervised Continual Learning (UCL)\nOnline Continual Learning (OCL)"
  },
  {
    "objectID": "slides/slides-continual-learning-beginners-guide.html#thank-you",
    "href": "slides/slides-continual-learning-beginners-guide.html#thank-you",
    "title": "Learning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning",
    "section": "Thank You",
    "text": "Thank You\n \nThank you for your attention!\n\n\nPlease feel free to ask any questions or reach out to me at:\nwangpengxiang@stu.pku.edu.cn"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html",
    "href": "slides/slides-architecture-based-continual-learning.html",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "",
    "text": "Continual Learning (CL): learning a sequence of tasks \\(t=1,\\cdots,N\\) in order, with datasets \\(D^t=\\{x^t, y^t\\}\\)\nTask-Incremental Learning (TIL): continual learning scenario, aim to train a model 𝑓 that performs well on all learned tasks\n\\[\\max_𝑓⁡ \\sum_{t=1}^N \\text{metric}(𝑓(x^t), y^t), \\{x^t, y^t\\} \\in D^t\\]\nKey assumptions when training and testing task \\(t\\):\n\nNo access to the whole data from previous tasks \\(1,\\cdots,t−1\\)\nTesting on all seen tasks \\(1,\\cdots,t\\)\nFor TIL testing, task ID \\(t\\) of each test sample is known by the model. Otherwise, it is task-agnostic testing\n\n\n\n\nReplay-based Approaches\n\nPrevent forgetting by storing parts of the data from previous tasks\nReplay algorithms use them to consolidate previous knowledge\n\n\nRegularization-based Approaches\n\nAdd regularization terms constructed using information about previous tasks to the loss function when training new tasks\n\n\nArchitecture-based Approaches \n\nDedicate network parameters in different parts of the network to different tasks\nKeep the parameters for previous tasks from being significantly changed\n\n\n\n\n\nOptimization-based Approaches\n\nExplicitly design and manipulate the optimization step\nFor example, project the gradient not to interfere previous tasks\n\n\nRepresentation-based Approaches\n\nUse special architecture or training procedure to create powerful representations\nInspired from self-supervised learning, large-scale pre-training like LLMs"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#problem-definition",
    "href": "slides/slides-architecture-based-continual-learning.html#problem-definition",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "",
    "text": "Continual Learning (CL): learning a sequence of tasks \\(t=1,\\cdots,N\\) in order, with datasets \\(D^t=\\{x^t, y^t\\}\\)\nTask-Incremental Learning (TIL): continual learning scenario, aim to train a model 𝑓 that performs well on all learned tasks\n\\[\\max_𝑓⁡ \\sum_{t=1}^N \\text{metric}(𝑓(x^t), y^t), \\{x^t, y^t\\} \\in D^t\\]\nKey assumptions when training and testing task \\(t\\):\n\nNo access to the whole data from previous tasks \\(1,\\cdots,t−1\\)\nTesting on all seen tasks \\(1,\\cdots,t\\)\nFor TIL testing, task ID \\(t\\) of each test sample is known by the model. Otherwise, it is task-agnostic testing"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#existing-approaches-for-til",
    "href": "slides/slides-architecture-based-continual-learning.html#existing-approaches-for-til",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "",
    "text": "Replay-based Approaches\n\nPrevent forgetting by storing parts of the data from previous tasks\nReplay algorithms use them to consolidate previous knowledge\n\n\nRegularization-based Approaches\n\nAdd regularization terms constructed using information about previous tasks to the loss function when training new tasks\n\n\nArchitecture-based Approaches \n\nDedicate network parameters in different parts of the network to different tasks\nKeep the parameters for previous tasks from being significantly changed"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#existing-approaches-for-til-1",
    "href": "slides/slides-architecture-based-continual-learning.html#existing-approaches-for-til-1",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "",
    "text": "Optimization-based Approaches\n\nExplicitly design and manipulate the optimization step\nFor example, project the gradient not to interfere previous tasks\n\n\nRepresentation-based Approaches\n\nUse special architecture or training procedure to create powerful representations\nInspired from self-supervised learning, large-scale pre-training like LLMs"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#architecture-based-approaches-1",
    "href": "slides/slides-architecture-based-continual-learning.html#architecture-based-approaches-1",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Architecture-based Approaches",
    "text": "Architecture-based Approaches\n\nLeverages the separability characteristic of the neural network architecture\nTreat the network as decomposable resources for tasks, rather than as a whole\nDedicate different parts of a neural network to different tasks to minimize the inter-task interference\nFocus on reducing representational overlap between tasks\n\nThe “part” of a network can be regarded in various ways:\n\nModular Networks: play around network modules like layers, blocks\nParameter Allocation: allocate group of parameters or neurons to task as a subnet\nModel Decomposition: decompose network from various aspects into shared and task-specific components"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#modular-networks-progessive-networks",
    "href": "slides/slides-architecture-based-continual-learning.html#modular-networks-progessive-networks",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Modular Networks: Progessive Networks",
    "text": "Modular Networks: Progessive Networks\n\n\n\n\nProgressive Networks, 2016\n\nExpand the network with new column module for each new task\nLinearly increasing model memory\nSimilar to independent training: train a independent network for each task"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#modular-networks-progessive-networks-1",
    "href": "slides/slides-architecture-based-continual-learning.html#modular-networks-progessive-networks-1",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Modular Networks: Progessive Networks",
    "text": "Modular Networks: Progessive Networks\n\n\n\n\nExpert Gate, 2017\n\nA new independent expert (network) for each new task\nSimilar to independent training but work in task-agnostic testing\nA gate works as the task ID selector at test time\nThe gate is a network learned through the task sequence"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#modular-networks-pathnet",
    "href": "slides/slides-architecture-based-continual-learning.html#modular-networks-pathnet",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Modular Networks: PathNet",
    "text": "Modular Networks: PathNet\nPathNet, 2017\n\nPrepare a large pool of modules for the algorithm to select from\nSeveral options in each module position, concatenated and form a subnet for a task\nChoose the path by tournament genetic algorithm between different paths during the training of a task"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-overview",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-overview",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: Overview",
    "text": "Parameter Allocation: Overview\nParameter Allocation\n\nRefines the level of modules to parameters or neurons\nSelects a collection of parameters or neurons to allocate to each task\nAlso forms a subnet for the task"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-overview-1",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-overview-1",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: Overview",
    "text": "Parameter Allocation: Overview\n\n\n\n\n\n\nWeight masks are way greater than feature masks in scale\nShould keep a decent amount of neurons in each layer\n\n\nParameter Allocation methods differ in ways:\n\nMethods to allocate\n\nManually set through hyperparameters\nLearned together with the learning process\n\nApplication of masks during training\n\nForward pass\nBackward pass\nParameter update step\n\nApplication of masks during testing\n\nMost methods fix the selected subnet after trained on their belonged task and use it as the only model to predict for that task during testing"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-packnet",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-packnet",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: PackNet",
    "text": "Parameter Allocation: PackNet\nPackNet, 2018\n\nSelect non-overlapping weight masks and allocate them to tasks\nFix masked parameters once trained until testing using the subnet\nPost-hoc selection by pruning (by absolute values of weights) after training\nRetraining after pruning as network structure changes\nManually allocation by percentage hyperparameters"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-den",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-den",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: DEN",
    "text": "Parameter Allocation: DEN\nDEN (Dynamically Expandable Networks), 2018\n\nFind the important neurons as feature masks for testing, and duplicate\nFind by training with equally L2 regularisation, whose connected parameters change a lot are important\nDynamic network expansion when performance can’t be improved, prune after\nThe training selects their own important neurons by L1 regularised training, then only train them by L2 regularisation\nManually allocation by threshold hyperparameters, slightly better than percentage"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-piggyback",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-piggyback",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: Piggyback",
    "text": "Parameter Allocation: Piggyback\nPiggyback, 2018\n\nLearnable allocation: binary masks are gated from real values which is differentiable and can be learned together with parameters\nStill binary during test\nSacrifices with the network parameters fixed, reduced representation ability\n\n\n\n\n\nSupSup, 2020\n\nExtends to task-agnostic testing"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-hat",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-hat",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: HAT",
    "text": "Parameter Allocation: HAT\nHAT (Hard Attention to the Task), 2018\n\nMasks and parameters are both learnable\nFix masked parameters once trained until testing using the subnet\nSparsity regularization for masks\n\n\n\n\n\nAdaHAT, 2024 (my work)\n\nAllow minor adaptive adjustment to masked parameters"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-cpg",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-cpg",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: CPG",
    "text": "Parameter Allocation: CPG\nCPG (Compacting, Picking and Growing), 2019\n\nPost-hoc pruning and retraining + network expanding + learnable masks (on previous weights)"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-ucl",
    "href": "slides/slides-architecture-based-continual-learning.html#parameter-allocation-ucl",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Parameter Allocation: UCL",
    "text": "Parameter Allocation: UCL\n\n\n\n\nUCL (Uncertainty-based Continual Learning), 2019\n\nIdentify the important neurons by uncertainty measure derived from Bayesian learning theory\nApply different regularisation to the weights by neuron importance\nthe important neurons only work in training\nThe identification of important neurons is soft controlled by coefficient hyperparameters (\\(\\sigma_{\\text{init}}\\)) in the regularisation terms\nMore like a regularisation-based but incorporate architecture-based ideas"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#model-decomposition-acl",
    "href": "slides/slides-architecture-based-continual-learning.html#model-decomposition-acl",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Model Decomposition: ACL",
    "text": "Model Decomposition: ACL\nACL (Adversarial Continual Learning), 2020\n\nShared and task-specific, modules, features\nShared module is adversarially trained with the discriminator to generate task-invariant features. The discriminator predicts task labels"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#model-decomposition-apd",
    "href": "slides/slides-architecture-based-continual-learning.html#model-decomposition-apd",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Model Decomposition: APD",
    "text": "Model Decomposition: APD\nAPD (Additive Parameter Decomposition), 2020\n\nDecomposes the parameter matrix of a layer mathematically:\n\n\\[\\theta_t =\\sigma \\odot \\mathcal{M}_t + \\tau_t,  \\mathcal{M}_t = \\text{Sigmoid}(\\mathbf{v}_t)\\]\n\nApply different regularisation strategies to shared \\(\\sigma\\) and task-specific \\(\\tau_t, \\mathbf{v}_t\\)\n\n\\[\n\\underset{\\boldsymbol{\\sigma}, \\boldsymbol{\\tau}_t, \\mathbf{v}_t}{\\operatorname{min}} \\mathcal{L}\\left(\\left\\{\\boldsymbol{\\sigma} \\otimes \\mathcal{M}_t+\\boldsymbol{\\tau}_t\\right\\} ; \\mathcal{D}_t\\right)+\\lambda_1\\left\\|\\boldsymbol{\\tau}_t\\right\\|_1+\\lambda_2\\left\\|\\boldsymbol{\\sigma}-\\boldsymbol{\\sigma}^{(t-1)}\\right\\|_2^2\n\\]\n\nShared parameters \\(\\sigma\\) not deviate far from the previous\nThe capacity of task-specific \\(\\tau_t\\) to be as small as possible, by making it sparse"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#model-decomposition-pgma",
    "href": "slides/slides-architecture-based-continual-learning.html#model-decomposition-pgma",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Model Decomposition: PGMA",
    "text": "Model Decomposition: PGMA\nPGMA (Parameter Generation and Model Adaptation), 2019\n\nTask-specific parameters \\(p_t\\) are generated by DPG (dynamic parameter generator)\nShared parameters \\(\\theta_0\\) (in solver \\(S\\)) adapt itself to task \\(t\\) with the generated task-specific \\(p_t\\)"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#challenge-network-capacity-and-plasticity",
    "href": "slides/slides-architecture-based-continual-learning.html#challenge-network-capacity-and-plasticity",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Challenge: Network Capacity and Plasticity",
    "text": "Challenge: Network Capacity and Plasticity\nNetwork Capacity Problem\n\nAny fixed model will eventually get full and lead to the performance drop, given the potentially infinite task sequence\nBecome explicit in architecture-based approaches\nCan be solved by taking shortcuts to expand the networks, but it is not fair\n\nStability-Plasticity Trade-Off\n\nContinual learning seeks to trade off the balance between stability and plasticity\nApproaches that fix part of model for previous tasks are lack of plasticity by stressing too much stability\nOthers whichever has task shared components still face the classic catastrophic forgetting problem, which is a result of lack of stability\nThey both lead to a bad average performance"
  },
  {
    "objectID": "slides/slides-architecture-based-continual-learning.html#thank-you",
    "href": "slides/slides-architecture-based-continual-learning.html#thank-you",
    "title": "Architecture-Based Approaches in Continual Learning",
    "section": "Thank You",
    "text": "Thank You\n \nThank you for your attention!\n\n\nPlease feel free to ask any questions.\n\n\nCheck out the post in my blog for complete narratives of this pre!"
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/logging.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/logging.html",
    "title": "Logging",
    "section": "",
    "text": "Configs about logging are in the configs/logger/."
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#csv-logger",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#csv-logger",
    "title": "Logging",
    "section": "1.1 CSV Logger",
    "text": "1.1 CSV Logger"
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#multiple-loggers",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#multiple-loggers",
    "title": "Logging",
    "section": "1.2 Multiple Loggers",
    "text": "1.2 Multiple Loggers\nThe key part is that, with a single self .log() method in the Lightning module, the result can be logged to different loggers at the same time.\nThat is to say, multiple loggers are supported. It is specified in logger config from the file in configs/logger/ folder. For example, in csv_tensorboard.yaml, there are csvand tensorboard appeared, which means the 2 loggers are applied."
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#organise-your-experiment-results",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#organise-your-experiment-results",
    "title": "Logging",
    "section": "2.1 Organise your experiment results",
    "text": "2.1 Organise your experiment results\n\nexperiment_name: the folder name show in your output log.\ntags: the tags shown in tags.log . See [tag system] tags.log records the tags for the experiment which were specified beforehand where your can write some script based on this to summarise under the same tag."
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#tips",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/logging.html#tips",
    "title": "Logging",
    "section": "2.2 tips",
    "text": "2.2 tips\nfrom them to configure and run different experiments in this program. Before you go, you need to know how to specify configurations (hyperparamters). I will help you understand the experiment procedure by explaining most of the configs. But you can also check the rich logs printed on console (also logs to a file in root folder called train.log) to be reminded what the program is doing."
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/backbone-network.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/backbone-network.html",
    "title": "How to Specify Backbone Network",
    "section": "",
    "text": "Backbone network refers to the feature extractor before output heads. It is implemented as PyTorch nn.module object. It is specified in the backbone entries in model config from the file in configs/model/ folder.\nTake a look at the example of model config: finetuning_mlp_til.yaml. There you can find backbone entries specifying the backbone network. Here it instantiates the nn.Module class MLP from the src/models/backbones/ folder. The instantiation works in the same way as aforementioned CL dataset, so please refer to that if you find it confusing.\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "Backbone Network"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/computing.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/computing.html",
    "title": "Get Started",
    "section": "",
    "text": "0.1 How to specify devices, batches and epochs\nConfigs related to computing are implemented as the Lightning Trainer object. They are specified in trainer config from the file in configs/trainer/ folder.\nTake a look at the example of trainer config – default.yaml. As the _target_ suggests, it instantiates the only Trainer class. The configs are set as the parameters of Trainer class that are called trainer flags.\nFor example, you can specify devices in the accelerator and devices flags, and epochs in the min_epochs and max_epochs flags.\nNote that batch size is usually a parameter of datamodule class (in dataset config from the file in configs/data/ folder), because it needs to be specified when constructing dataloaders.\nPlease refer to Trainer docs to know more about what computing configs can be set.\n\n\n0.2 How to specify optimiser and lr_scheduler\nThe optimization and learning rate schedule algorithm are implemented as PyTorch optimizer and lr_scheduler object. They are specified in the optimizer and scheduler entries in model config from the file in configs/model/ folder.\nTake a look at the example of model config – finetuning_mlp_til.yaml. There you can find optimizer and scheduler entries specifying the optimizer and lr_scheduler. As the _target_ suggests, it instantiates the optimizer class Adam and lr_scheduler class ReduceLROnPlateau from PyTorch built-ins.\nFor optimisers and lr_schedulers we often use PyTorch built-ins, but also could use from the custom classes from the src/models/optimizers/ and src/models/lr_schedulers/ folder. Note that some optimisation-based CL algorithms use different optimisers on different tasks.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/CL-dataset.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/CL-dataset.html",
    "title": "How to Specify CL Dataset",
    "section": "",
    "text": "Dataset is implemented as Lightning datamodule object. It is specified in dataset config from the file in configs/data/ folder.\nTake a look at the example of dataset config til_permuted_mnist.yaml. As the _target_ entries suggests, this config file targets to instantiate the Lightning datamodule class PermutedMNIST from the src/data/ folder. And we can tell from a line of code in src/data/__init__.py:\nfrom src.data.permuted_mnist import PermutedMNIST\nthat PermutedMNIST is in src/data/permuted_mnist.py. In the definition of the dataset class, all parameters of a dataset class must be specified from the entries of YAML config file. If not, an error would be raised. Note that there are also parameters with default values, which can be left unspecified and set to the defaults automatically.\nPlease refer to the docstring of a dataset class to understand the meaning of the parameters.\nPlease check out the src/data/ folder to see what datasets have been implemented.\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "CL Dataset"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/overriding-configs.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/overriding-configs.html",
    "title": "Config Your Experiments",
    "section": "",
    "text": "0.1 Quick config with override\nAnything you don’t wanna change in the original configs/data. You could add override arguments in experiments . For example, you could see in the example.yaml after\nAnother quicker way is to override in the command line arguments:\npython src/train.py experiment=example d\n\n、They are very useful if you want to specify run time. Like loggers, devices.\n\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "Overriding Configs"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/get-started.html",
    "href": "projects/continual-learning-arena/docs/get-started.html",
    "title": "Get Started",
    "section": "",
    "text": "A quick guide to setting up, running a continual learning experiment and check the results.\n\n1 Set Up\nAs it is a Python project on GitHub, clone the repository first.\ngit clone https://github.com/pengxiang-wang/continual-learning-arena\ncd continual-learning-arena\nInstall the required Python packages. It’s better to create an environment as there are large packages like PyTorch.\nconda create -n cl python=3.12 # [OPTIONAL] create conda environment\nconda activate cl\npip install -r requirements.txt\n\n\n2 Run Default Experiment\nTo run the default experiment, i.e. to train continual learning model with the default configuration:\npython src/train.py\n\n\nDefault Configuration\n\n\nCL Dataset: TIL (task-incremental learning), permuted MNIST, classification, 10 tasks;\nBackbone Network: MLP network structure, task-incremental heads (to align with TIL);\nCL Algorithm: simply initialise from last tasks (usually referred to as Finetuning or SGD);\nTraining:\n\n\nTo run your custom experiment, you need to create a YAML experiment configuration file in configs/experiment/ and specify the experiment argument following the run command:\npython src/train.py +experiment=example\nThe value of experiment argument should be the name of the config file (without .yaml) as this command runs the experiment specified in example.yaml. Go to Config Your Experiments to learn more about how to specify experiment configs in YAML files.\n\n\n3 Check the Results\nOnce the command above is executed, a folder containing all the information about the experiment is created in logs/example/runs/, named according to the time it was executed (It can include multiple runs if you execute the command several times). You can always check the results in this folder during the run.\nThe main results are:\n\n“config_tree.log” contains all the experiment configuration details;\n“test_metrics.csv” under the csv/ folder outputs the tested metrics, such as accuracy on each task and average accuracy over tasks.\n\nIf you set up some configs like loggers, saving checkpoints, profilers, …, other output files will show up in this folder.\n\n\n\n\n Back to top",
    "crumbs": [
      "Welcome",
      "Get Started"
    ]
  },
  {
    "objectID": "posts/continual-learning-metrics.html",
    "href": "posts/continual-learning-metrics.html",
    "title": "Understanding Metrics in Continual Learning",
    "section": "",
    "text": "The metrics in machine learning are sometimes confusing, not to mention that in continual learning with sequential tasks. Here is a note for understanding all the metrics in continual learning.\nThe metrics are generally divided into two categories:\nReal-time Monitor Metrics:\nMetrics for Calculation:\nAs for continual learning, it is actually an extension to normal machine learning. We start with the metrics for one task and then for the metrics over tasks."
  },
  {
    "objectID": "posts/continual-learning-metrics.html#monitor-training-batch-loss-accuracy",
    "href": "posts/continual-learning-metrics.html#monitor-training-batch-loss-accuracy",
    "title": "Understanding Metrics in Continual Learning",
    "section": "Monitor: Training Batch Loss / Accuracy",
    "text": "Monitor: Training Batch Loss / Accuracy\nIn training task \\(t\\), it is an important message for how well the model performs on current batch of data.\nThe loss of the batch is really handy to get as it’s needed for the backpropagation process. Basically every machine learning project takes it as the primary monitor shown in progress bar or dashboard or something.\nIt is a good indicator for monitoring:\n\nTell overfitting or underfitting from convergence behavior;\nTell if the learning rate too high or low;\nThe monitor to implement early stopping if you want it deployed in your project.\n\nIt doesn’t matter that the loss and accuracy are calculated from different batch, as the data batch is often shuffled and evenly distributed. Even if it’s not the case, they are a practical just for monitoring and getting a rough idea about the model in real time anyway.\n\n\n\n\n\n\nImplementation\n\n\n\nPrint or log after you get the loss variable calculated in every batch.\n\n\nSometimes the accuracy of a batch is also monitored because it is more intuitive than the loss to show the performance. It might introduce some extra calculation but very little.\n\n\n\n\n\n\nNote\n\n\n\n\nThe loss is typically referred to as classification loss (cross-entropy). Sometimes there are regularization terms as well, which we can also monitor.\nThe concept of accuracy is only for classification. For other tasks like regression, there are like MSE or something else."
  },
  {
    "objectID": "posts/continual-learning-metrics.html#monitor-cumulative-averages",
    "href": "posts/continual-learning-metrics.html#monitor-cumulative-averages",
    "title": "Understanding Metrics in Continual Learning",
    "section": "Monitor: Cumulative Averages",
    "text": "Monitor: Cumulative Averages\nCumulative averages over batches are often used instead of the immediate batch-specific metrics. They have advantages that the curves are smoothed and the overall training process is monitored.\n\n\n\n\n\n\nNote\n\n\n\nReset the cumulative counter at the start of each epoch. It makes it possible that the observation and comparison of changes between epochs, which is essential to plot learning curves.\n\n\n\n\n\n\n\n\nImplementation\n\n\n\nUse Torchmetrics."
  },
  {
    "objectID": "posts/continual-learning-metrics.html#calculation-validation-loss-accuracy",
    "href": "posts/continual-learning-metrics.html#calculation-validation-loss-accuracy",
    "title": "Understanding Metrics in Continual Learning",
    "section": "Calculation: Validation Loss / Accuracy",
    "text": "Calculation: Validation Loss / Accuracy\nThe validation process is to test the model trained so far during the training process without using the testing data. Apart from showing generalisation information, it offers a monitor for selecting the best model among the checkpoints during training.\nThe validation usually takes place after each epoch training, because that marks a milestone for the training process. Theoretically it could be after each batch training, but costs too much as it goes through a complete validation dataset.\nAs the validation dataset is large enough (usually above 10% split from training dataset), the validation process has to be done with batches. Monitoring the batch loss / accuracy makes no sense because all we want is a result. Therefore, the validation loss / accuracy are cumulative averages only for calculation.\n\n\n\n\n\n\nImplementation\n\n\n\nUse Torchmetrics."
  },
  {
    "objectID": "posts/continual-learning-metrics.html#monitor-learning-curves",
    "href": "posts/continual-learning-metrics.html#monitor-learning-curves",
    "title": "Understanding Metrics in Continual Learning",
    "section": "Monitor: Learning Curves",
    "text": "Monitor: Learning Curves\nWe’ve got the validation loss in the validation process after (let’s say) each epoch, which represents the performance of the trained model after the epoch, on validation dataset.\nWe’ve got the cumulative average training loss after each epoch, which represents the average performance of a series of models during the training process, on training dataset.\nThey are slightly different but we can still put them together to make a meaningful comparison. If we plot them epoch-wise, that’s what we call as learning curves. This is basically a rougher (updated each epoch) monitor than batch loss curve, but incorporating validation information which additionally shows the generalisation ability of the model.\n\n\n\n\n\n\nNote\n\n\n\nWe could plot learning curves batch-wise, but just as I said, it costs too much to validate after each batch of training."
  },
  {
    "objectID": "posts/continual-learning-metrics.html#calculation-testing-loss-accuracy",
    "href": "posts/continual-learning-metrics.html#calculation-testing-loss-accuracy",
    "title": "Understanding Metrics in Continual Learning",
    "section": "Calculation: Testing Loss / Accuracy",
    "text": "Calculation: Testing Loss / Accuracy\nThe testing process is to test the model after all the training and validating done using the testing data. It is a one-off thing and all we want is just a number.\nAs the testing dataset is large enough, the testing process has to be done with batches. Monitoring the batch loss / accuracy makes no sense because all we want is a result. Therefore, the testing loss / accuracy are cumulative averages only for calculation.\n\n\n\n\n\n\nImplementation\n\n\n\nUse Torchmetrics.\nAs this is only one value, use add_text in TensorBoard to prevent plotting curves with one point.\n\n\nIn some occasions, the model is tested on testing dataset after every training epoch or even batch. However, this breaks the rules of testing, so it is only used as illustrations in God’s eyes view instead of part of the testing procedure. Also be aware that it costs a lot."
  },
  {
    "objectID": "posts/songbook-abcjs-intro.html",
    "href": "posts/songbook-abcjs-intro.html",
    "title": "My Songbook Project with ABCjs",
    "section": "",
    "text": "📒 Go to Songbook\n\n\nThis is an introduction to my songbook project. Songbook is a collection of sheet music from songs. I really wanted to make my own digital songbook, and here it is!\nThere are lots of benefits to have my own songbook:\n\nGetting familiar with a song, without confusion about different versions.\nGrabbing handy sheet music or chords anytime when I feel like playing guitar, piano, accordion or just singing.\nMaking it a personal music library by adding other things like embedded videos, comments on practicing, background, …\n\nI’ve been exploring the way to build it. I tried a songbook PDF rendering system called Bard but it was too complicated and only for marking chords on lyrics. When I switched to Quarto for blogging, I also found a way to render music sheets – ABCjs which works pretty well in this blog.\nABCjs is a javascript framework to render music sheets written in ABC notation. ABC notation is known as the Markdown language for music sheet, so it can be easily incorporated in Markdown-based blogging system theoretically. For Quarto blogs, it can be rendered from raw javascripts from ABCjs. One thing to note is that they even provide audio features: player, MIDI, timbres, which makes a scorewriter software like Musescore come true in a browser. It’s a portable and lightweight system right in my blog, so that I don’t have to rely on any other softwares.\n\nABC notation documentation for how to encode music sheets: https://abcnotation.com/wiki/abc:standard:v2.2\nABC MIDI documentation for audio settings like timbres, volumes when the sheets are turned into audio: https://abcmidi.sourceforge.io\nABCjs documentation for how to add the elements made from ABC notation in webpages: https://paulrosen.github.io/abcjs/\n\nI have already set up the environment for this blog. Take your time to browse the types of music I enjoy, and, enjoy it! :)\n\n\n📒 Go to Songbook\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/cookbook-and-cooking-ideas-intro.html",
    "href": "posts/cookbook-and-cooking-ideas-intro.html",
    "title": "My Cookbook and Cooking Ideas",
    "section": "",
    "text": "🥘 Go to Cookbook\n\n\n🥘 Go to Cooking Ideas\n\n\nI used to learn cooking at home with my family when I was in China, tried and developed lots of learning notes and recipes. I’ll put them in the cookbook project.\nHowever, since I came to UK, I’ve been living on my own for some time. Without canteens that allow walking in to eat in a very low price, without any help from family, I found cooking and fridge management becomes a tricky topic of my everyday routine. On the other hand, the western food culture is so different that I have no idea how to cook at the beginning. I feel lost before the recipes in English. I have to learn some general knowledge first, then I discovered that at this stage I need some rough and handy inspirational ideas on what can be cooked with leftover ingredients in the fridge, rather than a bunch of detailed and perfected recipes. That’s why I made the cooking ideas project.\nFor the cooking ideas, they are organised by ingredients, gathering the following information in individual posts:\n\nSimplest Cooking: the simplest ways to cook, particularly helpful in the case of feeling tired but need to fill stomach. There are always cooking instructions in the food packages here in the UK, which are the main reference of this part.\n\nCondiment: it is usually plain and boring if the ingredients are cooked in the simplest way. Here are some condiment recommendations to add flavor.\nCooking Tips: additional cooking tips.\n\nClassic Dishes: the inspiration thoughts on how to cook the classical dishes with the ingredients.\nInnovative Dishes: the inspiration thoughts on some varied and innovative dishes with the ingredients.\nVarieties: introduce the varieties of the ingredient and their nuances.\nPurchase Advice: I will give some introductions on brands and advices if I have to.\n\nBefore we go, just a few things to know.\n\n\n\n\n\n\nGeneral Knowledge of Cooking\n\n\n\n\n1 teaspoon = 5 ml, 1 spoon = 15 ml\n\n\n\n\n\n\n\n\n\nGeneral Knowledge of Oven\n\n\n\n\nOven needs preheating.\nFoil and baking paper is for wrapping food in the oven to prevent it from losing moisture. It can also make cleanup easier to prevent food from sticking to the tray.\nBBQs and grills are different.\nGrills are meant to be outdoors. Some ovens provide grill mode or ==broiler==. We can also use grill pans.\nOven cooking uses middle shelf by default. Top shelf is ideal for broiling, browning, or quick cooking when a more intense heat is required, Bottom shelf is suitable for slow-cooking stews, casseroles, and dishes that require longer cooking times at lower temperatures.\n\n\n\n\n\n\n\n\n\nGeneral Knowledge of Storage\n\n\n\n\nDefrost could be in the fridge.\nDefrost thoroughly and use within 1 day.\nDo not refreeze once defrosted.\n\n\n\n\n\n🥘 Go to Cookbook\n\n\n🥘 Go to Cooking Ideas\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/English-learning-intro.html",
    "href": "posts/English-learning-intro.html",
    "title": "My Online English Learning Notebook",
    "section": "",
    "text": "📒 Go to Notebook\n\n\nI’ll put every English knowledge I’ve learned into this online notebook.\nThe most effective way I found to learn English expressions is by categories. Here I organize the expressions by different topics and put synonyms together to compare.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Quarto-intro.html",
    "href": "posts/Quarto-intro.html",
    "title": "A Brief Introduction to Quarto Blogging System",
    "section": "",
    "text": "There are so many features in Quarto, the system that I use for this blog site. I’ve gone through its documentations and took some brief and reorganized notes here for future references.\nQuarto is a quite comprehended digital content composing system, I would say. It manages the next step for a markdown to be turned into not only previews, but also proper PDFs, Microsoft Word documents, slides, and the most exciting, HTML into websites.\nThat means I don’t have to switch to any other softwares like Microsoft Office, Overleaf, for most of my own created contents, such as study notes, presentations. Above all, it is so convenient to put all of them together into my blog site and manage them at the same place, and generate whatever format I want from one Markdown source file alone.\nI’ve tried two systems for my blog, none of them fitting my need:\n\nA normal blog template called Chirpy. It was really hard for me to manage the HTML codes . I have to simply use glocal search to modify some elements shown in the template site, which makes no sense to me. Quarto is much better in terms of simplifying scary HTML stuff into YAML configs.\nA notes taking App called Obsidian. I was so fascinated with the knowledge system that they called Second Brain and stuff, and the App provides features that can link notes as knowledge graph. I switched to it to build a private knowledge system and quit blogging for a while, but sooner I just realized it was probably another pop psychology product and I don’t think I‘ve got enough time to build, manage and review the massive system. (I don’t even have time to manage a few blog posts!) I remember there is some called Zettelkasten which turns every piece of knowledge into single note, but now I think it makes things hard to access and the composer distracted if they are scattered around. Why not summarize knowledge together? As a result, I decided to return back to proper blog posts.\n\nLet’s dive into the Quarto features that I can make use of in my blog.\nFirst we talk about authoring syntax which is basically Markdown for single files, then how to render a single Markdown file alone into different formats including HTML, PDF… Finally, the structure of organizing multiple posts into a proper site and publishing.\n\nAuthoring\nQuarto use an extended Markdown of its own for authoring contents. Note that we have to adopt .qmd file extension instead of .md. Although they are compatible with each other, the Quarto system like VS Code extension won’t recognize .md. We should stick to .qmd, but there are some ways to treat it as .md like file associations in VS Code.\nAs my rules, I would stick to the original Markdown as much as possible in case of changing blogging systems. But there are still something extensions that I’d certainly use:\n\nCross references: everything can be marked with hashtags and referred to with @. Try to use this instead of links whenever necessary.\nCallout blocks: note it‘s got different syntax from Obsidian.\nEquations: definitely lots of maths in my academic posts. Note it’s the same as LaTeX editor using one dollar sign for inline and two dollar signs for displays.\nVideos: sometimes I do need to introduce videos in the posts for easy viewing, especially those related to cooking and music.\nBlock layout: contents can be organized as blocks just like figures and applied with layouts. For example, 2 columns text.\nArticle layout: any contents can make use of the page margin. This is super handy for secondary contents which I want to put aside, or a large block that doesn’t have to be squeezed in the middle.\nRaw contents using the original HTML: for example, text color. But it‘s not so handy to write a bunch of HTML tags so don’t be too generous to use that. I think it’s more important to know that there are way more potential usages on text format than we expect when we incorporating raw HTMLs. Refer to any HTML tutorials when I have any special needs.\nKeyboard shortcuts: I might use it in some technical tools posts.\n\nI should also know that some typical features are also extended:\n\nFigures: not only captioning and sizing, but aligning, adding a clickable link to a figure, lightboxes, sub-figures. Obviously, layout can also be applied to figures to create something like galleries.\nTables: support captioning, all kinds of styles and formats, sub-tables. Layout can be applied.\n\nLastly, note that some features are not supported:\n\nHighlighting texts with paired “==”\n\nAnd abundant features which I don’t wanna use at all:\n\nExecutable codeblocks: there is no need to make the codeblocks executable. This is an important feature of Quarto as they spend lots of space covering it, but I just gave up using it anyway.\nDiagrams: Creating diagrams by codes seems a bit dodgy. There are too many variations of diagrams in terms of the structure, not to mention their design elements. Like other Markdown-based notes taking system, Quarto Markdown provides Mermaid language which is difficult to learn, unsurprisingly.\nScholarly writing: I certainly don’t have to write my paper in a blog post!\n\n\n\nRendering\nThere are roughly 2 different ways to render a .qmd file into different formats: from terminals, from IDEs like VS Code and RStudio. I would choose VS Code as a seamless composing environment.\nAfter executing some command, we should get a file with same name as the .qmd file, but in the target format at the same directory.\nAll the supported formats can be found here, but the most common ones are HTML, PDF and Microsoft Office. HTML is meant for websites and we probably never generate a .html file alone (and it’s kind of confusing to be conceptualized as rendering as well), so I leave it to Web posting chapter.\nAll rendering options are configured at the beginning of a .qmd file, delimited by 3 dashes. They are YAML codes designed for configurations. Rendering options are specified in:\nformat:\n    html:\n        ...\n    pdf:\n        ...\n    docx:\n        ...\nwhere each format have their own field. I will go through the common options and then specific formats as follows.\nThe shared options includes table of contents (and depths), section numbering.\nAll PDF formatting options can be found here. Note that PDFs are actually generated by LaTeX engines (when you run the rendering, you can see some LaTeX auxiliary files flashing by). Therefore we should think in the way of LaTeX configs. For example, LaTeX engines, document class (controlling styles).\nMicrosoft Word file .docx formatting options can be found here. In PDF format styles are controlled by document class, while in this format by a reference template file. After the template being made, specify it in reference-doc field.\nSlides are a bit different from articles in terms of the content structure, but they can also be generated from Markdown (by marking the heading as ). Quarto has a complete system on slides contents. Microsoft Powerpoint file .pptx is the classic, but there is a much more handy Javascript framework called RevealJS for slides incorporated into Quarto. LaTeX beamer is also supported if we fancy it, but it is just unnecessary cos the math symbol rendering problem is already solved.\nSlides are definitely big enough topics deserving a few posts to cover. I’ll leave it to future.\nFinally, some miscellaneous authoring techniques for rendering PDF and other formats:\n\npage breaks\nmulti-format figures\n\n\n\nWeb posting\nWeb posting involves a group of .qmd documents and the way to publish them to a website. Before that, we first introduce the HTML rendering options.\nAll PDF formatting options can be found here. Besides the common options like toc and section numbers, there are many other customizable elements in a post page. For example, categories, table of contents, section numbering. You can also include raw HTMLs or javascripts, or apply CSS in in these option entries. Anyway, all the configs controlling a single post page can be found here.\nWhen it comes to a group of documents, we need a folder to accommodate them, and a proper project folder, which means some config files lie among the directory tree. They are YAML files, should be named as _metadata.yml if in sub-folders or _quarto.yml if in root directory. They are designed for specifying shared options across the posts in their directory, and we can imagine the priority order: configs in posts &gt; in _metadata.yml &gt; in _quarto.yml.\n_quarto.yml is important as it controls the whole website project. If we create a template website project from the Quarto command, we can see:\nproject:\n  type: website\n\nwebsite:\n    title: ...\n    navbar: ...\n\nformat:\n    html: ...\n(Ignore the project types for now as we only talk about websites. ) At the bottom are the global shared options mentioned earlier. In the website field, there are options controlling the overall elements of the website, for example, favicon, the navigation bar and the bar, where we can design tabs. The tabs can be a single post, a listing of posts or external links, so it’s pretty flexible to fit the need for a normal blog website. There’s too many things I just can’t cover, but if we need more inspirations about what the website can be designed into, check out the source project repo for the official Quarto page.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/AdaHAT/index.html",
    "href": "projects/AdaHAT/index.html",
    "title": "Paper: AdaHAT",
    "section": "",
    "text": "This is my first paper as first author, which was accepted by ECML PKDD 2024 conference. To find more information, please go to the links below.\nLinks:\n\nPublication page (SpringerLink)\nECML Accepted Papers page\n\nResources:\n\nPresentation slides\n\nLong version (by me)\nShort version (by Prof Jun Hong), presented on the conference\n\nPoster for the conference\nCode project\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/callbacks.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/callbacks.html",
    "title": "Logging Results",
    "section": "",
    "text": "0.1 How to add callbacks\n\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "Callbacks"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/CL-algorithm.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/CL-algorithm.html",
    "title": "How to Specify CL Algorithm",
    "section": "",
    "text": "Continual learning algorithm is implemented as a main part of Lightning module object. It is specified in model config from the file in configs/model/ folder.\nTake a look at the example of model config: finetuning_mlp_til.yaml. As the outmost _target_ entries suggests, it instantiates the Lightning module class Finetuning from the src/models/backbones/ folder. The instantiation works in the same way as aforementioned CL dataset, so please refer to that if you find it confusing.\n\n\n\n\n\n\nNote\n\n\n\nNote that continual learning algorithms typically have hyperparameters while this example of Finetuning algorithm does not have any.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "CL Algorithm"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/index.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/index.html",
    "title": "Config Your Experiments",
    "section": "",
    "text": "In this chapter, you will learn how to config your own experiments by simply specifying entries in the YAML config files. This functionality is powered by Hydra, a hierarchical YAML configs organising system.\n\n1 How Configs System Works\nAs said in Get Started, there are two ways to run:\npython src/train.py\npython src/train.py experiment=example\nThe training program src/train.py uses the default config from configs/train.yaml, where you can find many config entries for different types of things in the experiment. Some entries are quite straightforward with simple values, like experiment_name (string), seed(integer); some are very complicated topics like data, model. They cannot be configured within 1 entries, so they are separated as standalone YAML files in the subfolders. For example, CL-dataset:.\nThis hierarchical config structure is the key of Hydra makes the and we definitely do not want everything in one big config file,\nNote that separated configs should be included in the defaults list. That’s the syntax of Hydra and has nothing to do with “default” configuration.\nWe need to run experiment other than this default one. However,src/train.py and configs/train.yaml are linked unless we change the source code. In order to run others, we override train.yamlwith our custom configs in the configs/experiment/ folder.\nOverriding system\n\n\n2 What’s In the Configs?\nLet’s take a look at the default config train.yaml in the configs/experiment/ folder as an example to understand how it works, then you can create YAMLs in this same folder to set up your won experiments.\nIn the defaults list, each entries has a value that points to another YAML configuration file:\n\nDataset config: override /data specifies CL dataset via a config file from the configs/data/ folder. See CL dataset for details;\nModel config: override /model specifies the model used to train and test the dataset, including backbone neural network, CL algorithm and training options like optimiser and lr_scheduler via a config file from the configs/model/ folder. See Backbone Network, CL Algorithm and Training for details;\nTrainer config: override /trainer specifies Lightning trainer via a config file in configs/trainer/ folder. The trainer controls most of computing configs, including devices and epochs. See Computing for details;\nLogger config: override /logger specifies logging tools that we’d like to use for presenting results via a config file in configs/logger/ folder. They are loggers wrapped in Lightning APIs. See Logging for details.\nCallbacks config: override /callbacks specifies callbacks for Lightning module via a config file in configs/callbacks/. Callbacks provide non-essential logic embedded in the training and testing process. See Callbacks for details;\n\nYou can easily override any entries in the high-level config files in config/experiment without modifying those in configs/dataset, configs/model, etc. In the latter part of example.yaml, you can see these overrides in the list of data, model… See Overriding Configs for details.\nOther miscellaneous entries can be seen in the example as well: - seed: sets the global seed for reproductivity; - experiment_name, tags: for better organising especially when you come across lots of experiments; - …\nI will cover all of them in the sub-chapters.\n\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/training.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/training.html",
    "title": "Specifying Training Options",
    "section": "",
    "text": "After the dataset, network and algorithm are set, model training is started where there are lots of options, which includes:\n\nOptimiser i.e. optimisation algorithm and its hyperparameters learning rate\nLearning rate scheduler and its hyperparameters.\n\nThe optimization and learning rate schedule algorithm are implemented as PyTorch optimizer and lr_scheduler object. They are both specified in the optimizer and scheduler entries in model config from the file in configs/model/ folder.\nTake a look at the example of model config: finetuning_mlp_til.yaml. There you can find optimizer and scheduler entries specifying the optimizer and lr_scheduler. As the _target_ suggests, it instantiates the optimizer class Adam and lr_scheduler class ReduceLROnPlateau from PyTorch built-ins.\nFor optimisers and lr_schedulers we often use PyTorch built-ins, but also could use from the custom classes from the src/models/optimizers/ and src/models/lr_schedulers/ folder.\nNote that some optimisation-based CL algorithms use different optimisers on different tasks, but we haven’t come across that yet.\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "Training"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/docs/config-your-experiments/CL-scenario.html",
    "href": "projects/continual-learning-arena/docs/config-your-experiments/CL-scenario.html",
    "title": "How to Specify CL Scenario",
    "section": "",
    "text": "Continual learning has evolved into two major scenarios within the research context: Task-Incremental Learning (TIL) and Class-Incremental learning (CIL).\nIn this framework, the scenario is completely decided by the way incremental classifier heads evolve. The heads are the classifier after backbone network, and they are assigned to each task. When a new task comes, say it contains 10 classes and we have come across 10 classes, TIL creates a head for classifying the 10 classes alone, but CIL for classifying all the 20 classes, and the new 10s are considered as the 10ths.\n\n\n\n\n\nBoth incremental heads were implemented in this framework in src/models/heads/: HeadsTIL and HeadsCIL, they are both nn.Module class with some serial mechanism added.\nAs part of the model, it is in model config. Take a look at the example of model config: finetuning_mlp_til.yaml. There you can find heads entries specifying the incremental heads. Here it instantiates the HeadsTIL. The instantiation works in the same way as aforementioned CL dataset, so please refer to that if you find it confusing.\nNote that most algorithms are not for both scenarios. It is essential to specify the right one so that the algorithm works in a proper way.\n\n\n\n Back to top",
    "crumbs": [
      "Config Your Experiment",
      "CL Scenario"
    ]
  },
  {
    "objectID": "projects/continual-learning-arena/index.html",
    "href": "projects/continual-learning-arena/index.html",
    "title": "Welcome to Continual Learning Arena",
    "section": "",
    "text": "API Documentation\n   \n\nGo to Github Page\n\n\nThis is a machine learning framework written by me. It is the implementation of my PhD research work, AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning, which has been accepted for presentation at the ECML PKDD 2024 conference. Please find more information here.\nThis framework provides templates and environments for continual learning (CL) experiments. Continual learning is an area of machine learning that deals with learning new tasks sequentially without forgetting previous ones.\nThe framework includes the following implemented datasets and algorithms for CL currently. I’m working on incorporating as many of them as possible into this framework.\n\n\n\n\nCL Dataset\nDescription\n\n\n\n\nPermuted MNIST\nA MNIST variant for CL by random permutation of the input pixels to form differenet tasks\n\n\nSplit MNIST\nA MNIST variant for CL by spliting the dataset by class to form different tasks\n\n\nPermuted CIFAR10\nA CIFAR-10 permuted variant for CL.\n\n\nSplit CIFAR100\nA CIFAR-100 split variant for CL.\n\n\n\n\n\n\n\nCL Algorithm\nPublication\nCategory\nDescription\n\n\n\n\nFinetuning (SGD)\n-\n-\nSimply initialise from the last task.\n\n\nLwF [paper]\nArXiv 2016\nRegularisation-based\nMake predicted labels for the new task close to those of the previous tasks.\n\n\nEWC [paper]\nPNAS 2017\nRegularisation-based\nRegularisation on weight change based on their fisher importance calculated regarding previous tasks.\n\n\nHAT [paper][code]\nPMLR 2018\nArchitecture-based\nLearning hard attention masks to each task on the model.\n\n\nAdaHAT [code]\nECML PKDD 2024 (accept)\nArchitecture-based\nAdaptive HAT by managing network capacity adaptively with information from previous tasks.\n\n\n\n\nThe framework is developed from lightning-hydra-template, a general deep learning framework, but tailored for the paradigm of CL. The framework is powered by:\n\nPyTorch Lightning - a lightweight PyTorch wrapper for high-performance AI research. It removes boilerplate part of PyTorch code like batch looping, defining optimisers and losses, training strategies, so you can focus on the core algorithm part. It also keeps scalability for customisation if needed.\nHydra - a framework for organising configurations elegantly. It can turn parameters from Python command line into hierarchical config files, which is nice for deep learning as there are usually tons of hyperparameters.\n\n\n\n\n Back to top",
    "crumbs": [
      "Welcome",
      "Welcome Page"
    ]
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Shawn’s Slides Gallery",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nArchitecture-Based Approaches in Continual Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning with Non-Stationary Streaming Data: A Beginner’s Guide to Continual Learning\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "slides/slides-AdaHAT.html",
    "href": "slides/slides-AdaHAT.html",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "",
    "text": "Continual Learning\n\nA machine learning paradigm\nLearn continual tasks and adapt over time\nOne of the key features of human intelligence\n\nCatastrophic Forgetting\n\nDrastic performance drops on previous tasks after learning new tasks\nA major issue for continual learning algorithm to address\n\n\n\n\nContinual Learning (CL): learning a sequence of tasks \\(t=1,\\cdots,N\\) in order, with datasets \\(D^t=\\{x^t, y^t\\}\\)\n\n\nTask-Incremental Learning (TIL): continual learning scenario, aim to train a model 𝑓 that performs well on all learned tasks\n\\[\\max_𝑓⁡ \\sum_{t=1}^N \\text{metric}(𝑓(x^t), y^t), \\{x^t, y^t\\} \\in D^t\\]\nKey assumptions when training and testing task \\(t\\):\n\nNo access to the whole data from previous tasks \\(1,\\cdots,t−1\\)\nTesting on all seen tasks \\(1,\\cdots,t\\)\nFor TIL testing, task ID \\(t\\) of each test sample is known by the model\n\n\n\n\nReplay-based Approaches\n\nPrevent forgetting by storing parts of the data from previous tasks\nReplay algorithms use them to consolidate previous knowledge\n\n\nRegularization-based Approaches\n\nAdd regularization terms constructed using information about previous tasks to the loss function when training new tasks\n\n\nArchitecture-based Approaches\n\nDedicate network parameters in different parts of the network to different tasks (inherent nature of parameter separability)\nKeep the parameters learned in previous tasks from being significantly changed\nFocus on reducing representational overlap between tasks\n\n\n\n\n\nContinual learning is a trade-off between stability and plasticity.\n\nStability: preserve knowledge for previous tasks\nPlasticity: reserve representational space for new tasks\n\nWe must trade them off to get higher performance averaged on all tasks.\n\n\n\n\n\n\nFor replay, regularization approaches:\n\nEmphasis on stability in their forgetting prevention mechanisms\nBut generally still lean towards plasticity\n\n\n\nFor architecture approaches:\n\nDistinctly different strategies that overly prioritize stability\nTilting the trade-off towards stability instead"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#continual-learning",
    "href": "slides/slides-AdaHAT.html#continual-learning",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "",
    "text": "Continual Learning\n\nA machine learning paradigm\nLearn continual tasks and adapt over time\nOne of the key features of human intelligence\n\nCatastrophic Forgetting\n\nDrastic performance drops on previous tasks after learning new tasks\nA major issue for continual learning algorithm to address"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#problem-definition",
    "href": "slides/slides-AdaHAT.html#problem-definition",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "",
    "text": "Continual Learning (CL): learning a sequence of tasks \\(t=1,\\cdots,N\\) in order, with datasets \\(D^t=\\{x^t, y^t\\}\\)\n\n\nTask-Incremental Learning (TIL): continual learning scenario, aim to train a model 𝑓 that performs well on all learned tasks\n\\[\\max_𝑓⁡ \\sum_{t=1}^N \\text{metric}(𝑓(x^t), y^t), \\{x^t, y^t\\} \\in D^t\\]\nKey assumptions when training and testing task \\(t\\):\n\nNo access to the whole data from previous tasks \\(1,\\cdots,t−1\\)\nTesting on all seen tasks \\(1,\\cdots,t\\)\nFor TIL testing, task ID \\(t\\) of each test sample is known by the model"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#existing-approaches-for-til",
    "href": "slides/slides-AdaHAT.html#existing-approaches-for-til",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "",
    "text": "Replay-based Approaches\n\nPrevent forgetting by storing parts of the data from previous tasks\nReplay algorithms use them to consolidate previous knowledge\n\n\nRegularization-based Approaches\n\nAdd regularization terms constructed using information about previous tasks to the loss function when training new tasks\n\n\nArchitecture-based Approaches\n\nDedicate network parameters in different parts of the network to different tasks (inherent nature of parameter separability)\nKeep the parameters learned in previous tasks from being significantly changed\nFocus on reducing representational overlap between tasks"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#stability-plasticity-dilemma",
    "href": "slides/slides-AdaHAT.html#stability-plasticity-dilemma",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "",
    "text": "Continual learning is a trade-off between stability and plasticity.\n\nStability: preserve knowledge for previous tasks\nPlasticity: reserve representational space for new tasks\n\nWe must trade them off to get higher performance averaged on all tasks.\n\n\n\n\n\n\nFor replay, regularization approaches:\n\nEmphasis on stability in their forgetting prevention mechanisms\nBut generally still lean towards plasticity\n\n\n\nFor architecture approaches:\n\nDistinctly different strategies that overly prioritize stability\nTilting the trade-off towards stability instead"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#hat-hard-attention-to-the-task",
    "href": "slides/slides-AdaHAT.html#hat-hard-attention-to-the-task",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "HAT: Hard Attention to the Task",
    "text": "HAT: Hard Attention to the Task\n\n\nHAT (Hard Attention to the Task) is one of the most representative architecture-based approaches. Our work AdaHAT provides an extension to HAT.\n\n\nKey features:\n\nHard (binary) attention vectors (masks) on layers, dedicating the part of each task\nTreat the masks as model parameters, which means masks are learned\nMasks condition on gradients directly. Masked parameters won’t be updated"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#mechanism-details-of-hat",
    "href": "slides/slides-AdaHAT.html#mechanism-details-of-hat",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Mechanism Details of HAT",
    "text": "Mechanism Details of HAT\nLayer-wise attention vectors (masks) are learned to pay hard (binary) attention on units in each layer \\(l=1, \\cdots, L-1\\) to a new task \\(t\\):\n\\[\\textbf{m}^{\\le t}_l = \\max\\left(\\textbf{m}^t_l, \\textbf{m}^{\\leq t-1}_l\\right)\\]\nBinary values are gated from real-value task embeddings which is learnable: \\[\\mathbf{m}_l^t=\\sigma\\left(s \\mathbf{e}_l^t\\right)\\]\n\n\nMasks hard-clip gradients of parameters:\n\\[g'_{l,ij}= a_{l,ij} \\cdot g_{l,ij},\\ a_{l,ij} \\in \\{0, 1\\}\\]\n\\[a_{l,ij}  = 1-\\min\\left(m^{&lt; t}_{l,i},m^{&lt; t}_{l-1,j}\\right) \\]"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#problem-1-insufficient-network-capacity",
    "href": "slides/slides-AdaHAT.html#problem-1-insufficient-network-capacity",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Problem 1: Insufficient Network Capacity",
    "text": "Problem 1: Insufficient Network Capacity\n\n\nArchitecture-based approaches all suffer from network capacity problem especially in long sequence of tasks, sacrificing plasticity for stability.\nHAT’s hard-clipping mechanism allows no update for parameters masked by previous tasks.\n\n\nMore tasks come in\nMore active parameters become static\nLess sufficient network capacity\nLearning plasticity reduced, significantly affecting performance on new tasks"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#problem-1-insufficient-network-capacity-1",
    "href": "slides/slides-AdaHAT.html#problem-1-insufficient-network-capacity-1",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Problem 1: Insufficient Network Capacity",
    "text": "Problem 1: Insufficient Network Capacity\n\n\nHAT tries to solve it by sparsity regularization on learnable masks:\n\\[\\mathcal{L}'=\\mathcal{L}(f(x_t), y_t)+cR\\left(\\textsf{M}^t,\\textsf{M}^{&lt; t}\\right)\n\\] \\[\n    R\\left(\\textsf{M}^t,\\textsf{M}^{&lt;t}\\right)=\\frac{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}m_{l,i}^t\\left(1-m_{l,i}^{&lt;t}\\right)}{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}\\left(1-m_{l,i}^{&lt;t}\\right)}\n\\]\n\n\n\nMeant to promote low network capacity usage and high compactness of the masks\nHelps alleviate the issue on network capacity to a certain extent\nHowever, the network capacity will eventually run out"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#problem-2-non-adaptive-hyperparameters",
    "href": "slides/slides-AdaHAT.html#problem-2-non-adaptive-hyperparameters",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Problem 2: Non-adaptive Hyperparameters",
    "text": "Problem 2: Non-adaptive Hyperparameters\nMost architecture-based approaches:\n\nUse several hyperparameters to manually allocate network capacity usage\nWithout leveraging any information about previous tasks\n\n\nIn continual learning:\n\nWe never know how many tasks in future, maybe infinite …\nWe’re not able to decide the capacity allocation beforehand\nManual hyperparameter tuning is infeasible!\n\n\n\nIn our work, an adaptive strategy smartly allocates the network capacity with taking into account the information about previous tasks."
  },
  {
    "objectID": "slides/slides-AdaHAT.html#adahat-adaptive-hard-attention-to-the-task",
    "href": "slides/slides-AdaHAT.html#adahat-adaptive-hard-attention-to-the-task",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "AdaHAT: Adaptive Hard Attention to the Task",
    "text": "AdaHAT: Adaptive Hard Attention to the Task\nOur Proposed AdaHAT soft-clips gradients, which allows minor updates for parameters masked by previous tasks:\n\\[g'_{l,ij}=a^\\star_{l,ij} \\cdot g_{l,ij},  \\  a^\\star_{l,ij} \\in [0, 1]\\]\n\n\nThe adjustment rate \\(a^\\star_{l,ij}\\) now is an adaptive controller, guided by two pieces of information about previous tasks directly from HAT architecture:\n\nParameter Importance\nNetwork Sparsity"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#adahat-parameter-importance",
    "href": "slides/slides-AdaHAT.html#adahat-parameter-importance",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "AdaHAT: Parameter Importance",
    "text": "AdaHAT: Parameter Importance\nThe attention vectors (masks) indicate the importance of parameter.\n\nCumulative Attention Vectors (HAT)\n\\[\\textbf{m}^{\\le t}_l = \\max\\left(\\textbf{m}^t_l, \\textbf{m}^{\\leq t-1}_l\\right)\\]\n\nBinary \\(\\{0, 1\\}\\), represents if it’s masked by previous tasks\n\n\n\nSummative Attention Vectors (AdaHAT)\n\\[\\textbf{m}^{\\leq t,\\text{sum}}_l = \\textbf{m}^t_l +  \\textbf{m}^{\\leq t-1, \\text{sum}}_l\\]\n\nRange from \\(0\\) to \\(t-1\\), represents how many previous tasks it’s masked\nEncapsulates more information about previous tasks\n\n\n\nAdaptive process: Higher summative vectors  More important to previous tasks  Smaller adjustment rate \\(a^\\star_{l,ij}\\)  Smaller updates for the parameter"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#adahat-network-sparsity",
    "href": "slides/slides-AdaHAT.html#adahat-network-sparsity",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "AdaHAT: Network Sparsity",
    "text": "AdaHAT: Network Sparsity\nThe sparsity regularization term \\(R\\left(\\textsf{M}^t,\\textsf{M}^{&lt;t}\\right)\\) measures the compactness of masks.\nIt is closely related to the current network capacity:\n\n\n\nGenerally, when a smaller proportion of parameters in the network are static (i.e., sufficient network capacity), the regularization value tends to be larger, as there is a great possibility for the hard attention to be paid to active parameters.\n\n\n\nAdaptive process: Higher sparsity regularization  (Suggesting) more unmasked space available for new tasks  Less need to adjust the static space for previous tasks, should go for active parameters  Smaller adjustment rate \\(a^\\star_{l,ij}\\) in general"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#adahat-the-adjustment-rate",
    "href": "slides/slides-AdaHAT.html#adahat-the-adjustment-rate",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "AdaHAT: The Adjustment Rate",
    "text": "AdaHAT: The Adjustment Rate\n\nAdaptive Adjustment Rate (AdaHAT)\n\\[ a^\\star_{l,ij} = \\frac{r_l}{\\min\\left(m^{&lt; t, \\text{sum}}_{l,i},m^{&lt; t, \\text{sum}}_{l-1,j}\\right)+r_l},\\ r_l = \\frac{\\alpha}{R\\left(\\textsf{M}^t,\\textsf{M}^{&lt;t}\\right) + \\epsilon}\\]\n\n\nAdjustment Rate (HAT)\n\\[a_{l,ij}  = 1-\\min\\left(m^{&lt; t}_{l,i},m^{&lt; t}_{l-1,j}\\right) \\]\n\n\nThe adjustment rate in AdaHAT adaptively incorporates both information about previous tasks:\n\nThe higher parameter importance \\(\\min\\left(m^{&lt; t, \\text{sum}}_{l,i},m^{&lt; t, \\text{sum}}_{l-1,j}\\right)\\), the lower adjustment rate\nThe higher network sparsity \\(R\\left(\\textsf{M}^t,\\textsf{M}^{&lt;t}\\right)\\), the higher adjustment rate\n\nWhile in HAT only the accumulation of masks."
  },
  {
    "objectID": "slides/slides-AdaHAT.html#adahat-adaptive-hard-attention-to-the-task-1",
    "href": "slides/slides-AdaHAT.html#adahat-adaptive-hard-attention-to-the-task-1",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "AdaHAT: Adaptive Hard Attention to the Task",
    "text": "AdaHAT: Adaptive Hard Attention to the Task"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#main-results",
    "href": "slides/slides-AdaHAT.html#main-results",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Main Results",
    "text": "Main Results"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#main-results-1",
    "href": "slides/slides-AdaHAT.html#main-results-1",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Main Results",
    "text": "Main Results\n\n\n\nDatasets: Permuted MNIST, Split CIFAR-100, 20 tasks\nMain metrics:\n\nAverage Accuracy (AA) over all tasks\nForgetting Rate (FR)\n\nMetrics for stability-plasticity trade-off:\n\nBackward Transfer (BMT) for stability\nForward Transfer (FWT) for plasticity\n\n\n\nResults:\n\nAdaHAT outperforms all baselines\nAdaHAT balances stability-plasticity better, while\n\nHAT: high BWT, low FWT\nFinetuning: low BWT, high FWT\nHAT-const-1: low BWT, high FWT\n\n\n\n\n\n\nConclusions: it is important to maintain a balanced stability-plasticity trade-off for optimal performance."
  },
  {
    "objectID": "slides/slides-AdaHAT.html#results-on-longer-task-sequences",
    "href": "slides/slides-AdaHAT.html#results-on-longer-task-sequences",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Results on Longer Task Sequences",
    "text": "Results on Longer Task Sequences\n\n\n\n\nDataset: Permuted MNIST, 50 tasks (longer)\n\n\nResults:\n\nHAT slightly outperforms before task 8 then drastically drops\nAdaHAT keeps significant superiority after the turning point\nAdaHAT is still close to HAT before task 8\n\n\n\nConclusions:\n\nThere is a turning point for HAT when it exhausts network capacity\nAdaHAT mimics HAT well before the network capacity limit, and shows much more capability for long task sequence settings"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#network-capacity-usage",
    "href": "slides/slides-AdaHAT.html#network-capacity-usage",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Network Capacity Usage",
    "text": "Network Capacity Usage\n\n\n\n\n\nNetwork Capacity Measurement\n\\[NC = \\frac{1}{\\sum_{l} N_l}\\sum_{l,i,j} a_{l,ij}\\]\n\n\n0 = all parameters can be updated freely\n1 = no parameter can be updated\n\n\n\nResults and Conclusions:\n\nHAT runs out of network capacity very soon at a fixed turning point (task 8)\nAdaHAT again behaves very similarly to HAT at first\nAfter the turning point, it manages it adaptively over time (through an adaptive adjustment rate), make it converge to 0 but never reach 0"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#ablation-study",
    "href": "slides/slides-AdaHAT.html#ablation-study",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Ablation Study",
    "text": "Ablation Study\n\n\n\n\nAblation of two pieces of information:\n\nAdaHAT-no-sum: fix summative \\(\\min\\left(m^{&lt; t, \\text{sum}}_{l,i},m^{&lt; t, \\text{sum}}_{l-1,j}\\right)\\) at constant \\(t\\)\nAdaHAT-no-reg: fix regularization term \\(R\\left(\\textsf{M}^t,\\textsf{M}^{&lt;t}\\right)\\) at constant 0\n\n\n\nResults: both underperform AdaHAT but outperform HAT\n\n\nConclusions: both information (parameter importance, network sparsity) play crucial roles for an adaptive extension of HAT."
  },
  {
    "objectID": "slides/slides-AdaHAT.html#hyperparameters",
    "href": "slides/slides-AdaHAT.html#hyperparameters",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\n\n\n\nAdaHAT introduces only one additional hyperparameter:\n\n\n\\(\\alpha\\) – overall intensity of gradient adjustment\n\n\nResults: \\(\\alpha=10^{-6}\\) is optimal.\n\n\nConclusions:\n\nNeither small nor large gradient adjustment balances the stability-plasticity trade-off, thus underperforms\nThe optimal is still a relatively small value, indicating the importance to design a proper and well-guided adjustment rate"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#conclusions-1",
    "href": "slides/slides-AdaHAT.html#conclusions-1",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Conclusions",
    "text": "Conclusions\nExisting architecture-based approaches (like HAT):\n\nTend to tilt the stability-plasticity trade-off towards stability\nSuffer from insufficient network capacity problem in long sequence of tasks\n\nOur proposed AdaHAT:\n\nBalances the trade-off in an adaptive adjustment mechanism\nAlso retains maximum stability benefits before the network capacity limit\nEffectively leverages information about previous tasks which was seldom used in architecture-based approaches\nAll of them leads to better performance than HAT\n\nFuture work:\n\nExplore and exploit more subtle information about previous tasks"
  },
  {
    "objectID": "slides/slides-AdaHAT.html#thank-you",
    "href": "slides/slides-AdaHAT.html#thank-you",
    "title": "AdaHAT: Adaptive Hard Attention to the Task in Task-Incremental Learning",
    "section": "Thank You",
    "text": "Thank You\n \nThank you for your attention!\n\n\nPlease feel free to ask any questions or reach out to us at:\nwangpengxiang@stu.pku.edu.cn"
  },
  {
    "objectID": "cv/cv-draft.html",
    "href": "cv/cv-draft.html",
    "title": "Shawn's Blog",
    "section": "",
    "text": "Languages and Tools - proficient: Python, R, Bash intermediate: Spark, SQL, Tensorflow/Keras, Git, Docker\nSelected Coursework - Deep Learning, Bayesian Machine Learning, Big Data System with Spark, Linear Models for Data Science, Statistical Learning, Data Engineering, Linear Algebra, Econometrics I & II, Data Visualization, Programming in Python\n\n\nDelivered 30+ advanced analytical studies (MTA, drivers analysis, diff-in-diff) to high profile clients including 15+ to marketing teams at Google and Meta\nBuilt diff-in-diff and multi-touch attribution model codebase in Python(OOP) for Ad Data Science Team. This codebase has saved 100+ hours and brought over in $2 million in revenue to date\n\n\n\nSolved entity resolution problem with Gradient Boosted Trees linking sparse wikipedia references to Internet Archive Database References\nImplemented model in Python which, given a wikipedia reference, will return a match in the Internet Archive Database along with a probability of matching.\n\n\n\nDeveloped and automated wave-over-wave statistical tests on time series data in 17 tracker surveys of 5 countries\nLed project to build a Python Web Bot (Selenium) to automate generation of test cases in surveys, contributed this to data science code base (used by 60+ data scientists)\nOver 300+ requests, pulled data from API or large database into R, wrangled data using R, and output figures and tables\n\n\n\nLed in modeling projects predicting election turnout for entire U.S. in 2022, phone response rates, etc.\nWrangled, cleaned, weighted, or made presentations for 60+ survey datasets with R, SQL, and AWS\nUsing R Shiny, built a codeless-crosstab tool for company’s research team\n\n\n\nDesigned and executed multiple survey experiments in original research projects\nMentored 60+ students in solving econometrics problems in weekly office hours\nVisualized data using R creating 50+ informative figures for AFS official report , news outlets, and professor’s projects\n\n\n\nCompared and estimated several Bayesian Regression models with PYMC3 in Python. Ultimately, used hierarchal negative binomial model to predict shootings and make inferences about gun laws\n\n\n\nrecieved award from Political Science department to fund a panel survey (n=1000).\nestimated linear regression and multinomial logit in observational study context and wrote paper \n\n\n\n\n Back to top"
  },
  {
    "objectID": "cv/cv-en.html",
    "href": "cv/cv-en.html",
    "title": "Shawn's Blog",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/meat/eggs.html",
    "href": "topics/cooking-ideas/ingredients/meat/eggs.html",
    "title": "Eggs",
    "section": "",
    "text": "Simplest Cooking\n\nPan fry:\nBoil:\n\n\n\n\n\n\n\nCondiment\n\n\n\nNo need to season because it’s salty enough.\n\n\n\n\nClassic Dishes\n\nFull Breakfast\nSandwiches:\n\nEgg sandwich\n\n\nEgg Info is a good site collecting recipes for eggs.\n\n\nVarieties\n\nBy size: medium / large\nFree range\n\n\n\nStorage\n\nKeep refrigerated for freshness.\nFor best results remove from fridge 30 mins before use.\nMeat is freezable. Once frozen use within 2 months.\nWhen spoiled: smells acidic, grey coloured.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/meat/pork-sausage.html",
    "href": "topics/cooking-ideas/ingredients/meat/pork-sausage.html",
    "title": "Pork Sausages (British)",
    "section": "",
    "text": "We talk about British pork sausages.\n\nSimplest Cooking\n\nOven: 190 °C middle shelf, 20 mins for small ones (30 mins for larger), turn halfway through\n\n\n\nClassic Dishes\nSausages are very British and usually appear in British dishes.\n\nBangers and Mash: sausages and mashed potato served with peas\nPigs in Blankets: wrap sausage with bacon rashers, oven cook\nToad in the Hole: oven cooked sausages with Yorkshire pudding batter. Serve with peas\n\n\n\nVarieties\n\nCumberland sausage\nLincolnshire Sausage\nChipolata\n\n\n\nStorage\n\nFresh meat needs to be kept refrigerated. Once opened use within 2 days.\nMeat is freezable. Once frozen use within 2 months.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/meat/beef-mince.html",
    "href": "topics/cooking-ideas/ingredients/meat/beef-mince.html",
    "title": "Beef Mince",
    "section": "",
    "text": "Beef mince, also known as ground beef.\n\nSimplest Cooking\n\nPan fry (most common): a little bit oil, high heat to make a crust, split and stir thoroughly to avoid steaming, 5 mins, until no pink\n\n\n\n\n\n\n\nCondiment\n\n\n\nSalt and pepper.\n\n\n\n\n\n\n\n\nCooking Tips\n\n\n\n\nMince can be shaped into meatballs, patty, meatloafs. We have separate chapters for them\nAdditional fats can be disposed of.\n\n\n\n\n\nClassic Dishes\n\nPasta:\n\nBolognese Spaghetti: beef mince, tomatoes with vegetables\nBeef Lasagna: beef mince, tomatoes, cheese in between lasagna sheets, oven cooked\n\nCottage Pie: beef mince thickened\nMeatloaf: Oven cooked meatloaf mixed with ingredients like onions and oats, glazed with ketchup, serve with mashed potatoes and vegs\n\n\n\nVarieties\n\nBy fat percentage (20%, 5%)\n\n\n\nStorage\n\nFresh meat needs to be kept refrigerated. Once opened use within 1 day.\nMeat is freezable. Once frozen use within 3 months.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/noodles-bowls.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/noodles-bowls.html",
    "title": "Pizza",
    "section": "",
    "text": "wasabi chicken teriyaki yakisoba\nchargrill\nStir and tuck in.\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/baby-potatoes.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/baby-potatoes.html",
    "title": "Mixed Vegetables (Ready)",
    "section": "",
    "text": "pierce bags microwave 7 mins\nleave to cool mix with a large teaspoon of creme fraiche Chopped spray onions for tasty, quick potato salad . You could also add some chunky pieces of peeled avocado\nkeep re\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/pizza.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/pizza.html",
    "title": "Pizza",
    "section": "",
    "text": "Simplest Cooking\n\nOven: On a baking tray, 240 °C (the higher the better) middle shelf, 10 min from chilled (13 mins from frozen).\n\n\n\nStorage\nKeep refrigerated. Once opened use within 1-2 days. Freezable. Once frozen use within 1 month.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/Chinese-dumplings.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/Chinese-dumplings.html",
    "title": "Chinese Dumplings",
    "section": "",
    "text": "Simplest Cooking\n\nBoil (most common): boil water, add the dumplings, wait for water boiling again, then cook for 6 mins, stir occasionally. Either drain or not is fine.\nPan shallow fry: a little bit oil, fry 5 mins, turn frequently, then add boiling water to half cover the dumplings, put on the lid and simmer for 5 mins. Drain off the water.\nSteam: 15-20 min.\n\n\n\n\n\n\n\nCooking Tips\n\n\n\n\nSuggested water amount for boiling: 1.5 litre water per 400g dumplings.\n\n\n\n\n\nStorage\n\nChinese dumplings need to be kept frozen.\n\n\n\nPurchase Advice\n\nFreshasia\nKung Fu Food\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/ready-meals/rice-bowls.html",
    "href": "topics/cooking-ideas/ingredients/ready-meals/rice-bowls.html",
    "title": "Pizza",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/vegetables/mixed-vegetables-stir-fry.html",
    "href": "topics/cooking-ideas/ingredients/vegetables/mixed-vegetables-stir-fry.html",
    "title": "Mixed Vegetables (Stir Fry)",
    "section": "",
    "text": "There are packed mixed vegetables ready to stir fry which you can find in British supermarkets.\n\nSimplest Cooking\n\nStir fry: Preheat the wok or pan over a high heat. Add 10ml of oil, stir fry for 6 mins. Stir continuously. If necessary, add a splash of water to prevent sticking and burnt.\n\n\n\n\n\n\n\nCooking Tips\n\n\n\n\nThe wok or pan should be large enough to get the veggies enough room to move around.\n\n\n\n\n\nVarieties\n\nTenderstem broccoli + mangetout + mixed vegetables (pak choi + carrot + salad onion)\nMushroom + bean sprouts + cabbage + carrot + onion\n\n\n\nStorage\n\nKeep refrigerated. Once opened use within 1 day.\nNot freezable.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/vegetables/Tenderstem-broccoli.html",
    "href": "topics/cooking-ideas/ingredients/vegetables/Tenderstem-broccoli.html",
    "title": "Tenderstem Broccoli",
    "section": "",
    "text": "Tenderstem broccoli is a variety of broccoli that has long, tender stems. Find more information at their official website.\n\nSimplest Cooking\nWash before use.\n\nBoil: boiling water 4 mins. For extra tenderness boil it for a little bit longer.\nStir fry: chop into smaller chunks. Add oil, stir fry 5 mins, add in garlic slices and soy sauce. Until bright green and tender.\n\n\n\n\n\n\n\nCooking Tips\n\n\n\n\nIf you like to retain a crunchy bite, make sure you don’t boil it for too long at all.\nHave a sharp knife or a fork on hand to give the stem a prod to see if it’s cooked to your liking.\n\n\n\n\n\nInnovative Dishes\n\nStir-fry Tenderstem with Bacon: cut bacon rashers into 3 pieces, simply stir-fry adding in garlic and soy sauce. An easy way to make some stir-fry veg and meat that tastes like Chinese food in the UK.\n\n\n\nPurchase Advice\nIt can be found in all British supermarkets.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/snacks/miscellaneous.html",
    "href": "topics/cooking-ideas/ingredients/snacks/miscellaneous.html",
    "title": "Shawn's Blog",
    "section": "",
    "text": "鸡块零食 (chicken bites) | keep re, | 2天 | 不行 | | use by | | | | | | | | |fridge raiders\ncrepe | | | | | best before | | | | | | | | |\n辣条 Soybean sticks"
  },
  {
    "objectID": "topics/cooking-ideas/ingredients/snacks/miscellaneous.html#others",
    "href": "topics/cooking-ideas/ingredients/snacks/miscellaneous.html#others",
    "title": "Shawn's Blog",
    "section": "",
    "text": "鸡块零食 (chicken bites) | keep re, | 2天 | 不行 | | use by | | | | | | | | |fridge raiders\ncrepe | | | | | best before | | | | | | | | |\n辣条 Soybean sticks"
  },
  {
    "objectID": "topics/songbook/songs/I-Need-You.html",
    "href": "topics/songbook/songs/I-Need-You.html",
    "title": "每一天我需要祢",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V-V-PC-PC-C | Interlude-(+3)PC-C-C-C-Outro\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/I-Am-Strengthened-in-Him.html",
    "href": "topics/songbook/songs/I-Am-Strengthened-in-Him.html",
    "title": "滿有能力",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V-C | Interlude V-C | B-B | (+2) C-C(repeat last)-Outro\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/You-Are-Worthy-Holy-Holy-All-Nations-Sing.html",
    "href": "topics/songbook/songs/You-Are-Worthy-Holy-Holy-All-Nations-Sing.html",
    "title": "祢是配得 (聖哉聖哉全地唱)",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: C1-C1-C2 | B-B | C2-C2-C1(soft)| V(repeat last)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/Marks-of-Grace.html",
    "href": "topics/songbook/songs/Marks-of-Grace.html",
    "title": "恩典的記號",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: V1-V2-C1-C2 | Interlude-V2-C1-C2-C1-C2\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/Morning-to-Night.html",
    "href": "topics/songbook/songs/Morning-to-Night.html",
    "title": "從早晨到夜晚",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\nStructure: Intro-V-Interlude-V-Interlude-C-Interlude-Interlude-C-B-B-Interlude2-C-(+2)C-C-Outro\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/songbook/songs/Amazing-Grace.html",
    "href": "topics/songbook/songs/Amazing-Grace.html",
    "title": "Amazing Grace",
    "section": "",
    "text": "+1 -1 Key to C Reset Key +Octave -Octave\n\n\n\n\n\n\n\n Back to top"
  }
]