---
title: Config Your Optimizer
---

We use the optimizer to train task in the framework of PyTorch and Lightning. As continual learning is a multiple task, It should have each optimizer for each task in continual learning.  We could apply one optimizer to all tasks or assign each task a specific optimizer.

::: { .callout-note }
As for [optimization-based approaches](../../../../posts/continual-learning-beginners-guide.qmd#optimization-based-approaches) design the mechanism in terms of optimization, usually take different optimizers. However, this optimization evolving can be defined in the CL algorithm and don't have to define differnet optimizers manually in the config.

:::

# Single Optimizer

To configure the optimizer for all tasks for your experiment, link the `optimizer` field in the main YAML file to sub-config file of the directory [configs/optimizer/] , and specify the class and its arguments in the sub-config. In this package, we just use the PyTorch built-in optimizers. They are useful enough and we don't have  to implement any other. But just in case you do have the special need custom your own `torch.optim.Optimizer` class, you can also implement and specify it for your experiment.

Here is an example:

```
./clarena/example_configs
├── __init__.py
├── example.yaml
├── optimizer
│   └── sgd.yaml
...
```

```{.yaml filename="example_configs/example.yaml"}
defaults:
  ...
  - optimizer: sgd.yaml
  ...

```

```{.yaml filename="example_configs/optimizer/sgd.yaml"}
_target_: torch.optim.SGD
_partial_: True # partially instantiate optimizer without 'params' argument
lr: 0.01

```

::: {.callout-danger }
Make sure you include `_partial_=True` as the PyTorch optimizer need the model parameter as an argument to be fully instantiated and we now in the phase of configuration certainly don't have that argument, the optimizer is partially instantiated.
:::

# Multi optimizer

nd specify the list `defaults` of optimizers you want to use in the sub-config.



```
./clarena/example_configs
├── __init__.py
├── example.yaml
├── optimizer
│   └── sgd.yaml
...
```

```{.yaml filename="example_configs/example.yaml"}
defaults:
  ...
  - optimizer: sgd.yaml
  ...

```

```{.yaml filename="example_configs/optimizer/sgd.yaml"}
_target_: torch.optim.SGD
_partial_: True # partially instantiate optimizer without 'params' argument
lr: 0.01

```




# Supported Optimizers {#list}


We fully supported every optimizers defined in PyTorch, so please refer to here to see the full list. To select a optimizer, replace with the class name. Please refer to [PyTorch documentation](https://pytorch.org/docs/stable/optim.html#algorithms) of each class to know what fields are required for the class.


