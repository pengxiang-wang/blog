---
title: "Continual Learning + Machine Unlearning"
author:
    - Pengxiang Wang
institute:
    - Peking University, School of Mathematical Sciences
    - University of Bristol, School of Engineering Mathematics and Technology
date: 2024-10-28
navigation: horizontal
aspectratio: 169
header-includes: |
  \setbeamertemplate{footline}[frame number]
  \titlegraphic{
    \centering
    \includegraphics[height=0.8cm]{../assets/pku-logo.png} \hspace{0.5cm}
    \includegraphics[height=0.8cm]{../assets/uob-logo.png}
  }
section-titles: true
theme: default
format:
    beamer:
        toc: false
        incremental: false
toc: true
image: assets/CLPU.png
categories: [research]
---

# Machine Unlearning

## Machine Unlearning Motivation

What is **machine unlearning**:

> **Machine unlearning** is the process of deliberately removing specific data from a machine learning model to ensure that the removed data no longer influences the model’s predictions -- an undo option of machine learning process.


Data Deletion:

- Traditionally: delete from databases
- AI: delete both from back-end databases and from trained models

Application Movitation:

- **Privacy**: 
  - Regulations: GDPR, CCPA, etc. when the user withdraw the consent, "the right to be forgotten"
  - Delete the requested data by users
- **Security**: 
  - Adversarial attacks are possible to extract private information from the trained model. E.g., model inversion attacks, membership inference attacks, etc.
  - Delete the adversial data
- **Data Quality**: 
- Delete unwanted data, e.g., outdated data, noisy data, biased data, etc.


## Machine Unlearning Framework

![Machine Unlearning Framework](assets/machine-unlearning-framework.png)


## Formal Definition

D = Dr+ Df

Df: forget set

Assumptions: 

- The unlearning data are not big. Practically considering, also Otherwise, it is easier to do retraining.

## Retraining

The problem makes unlearning difficult:

- Neural networks parameters do not tend to show any clear connection to the training data. AI models have to be considered as a whole.
- Stochasticity and Incrementality of training
- 这种整体性很容易导致unlearning过头，catastrophic unlearning, reduce performance


**Retraining:**
- Delete target data and re-train the model with the rest of data from scratch
- A naive way, but not always feasible
- Achieves upper bound 


The problem of retraining:
- Doesn't worth, computation cost
- Not always having aceess to all training data


## Methodology

Scenarios
- Data Deletion
- Class Removal


- Model-Agnostic or Model-Intrinsic
- Data-Driven Approaches, most model-agnostic

## Method: SISA

Data Partitioning (Efficient Retraining)

SISA (Sharded, Isolated, Sliced, Aggregated), 2021:

- Isolate: Isolate network and slice data into shards
- build up correspondance bewteen divided network and data
- Retraining the corresponding network of the data shard to be forgotten

Fractioning the retraining into smaller units

## Method: SISA

![SISA](assets/SISA.png)


## SISA: Technical Details

- Slicing screenshots
- Save the state of parameters of slices
- Training: a) first locate the slice in which du is located, referred to as Dk,u, and (b) perform the training procedure as specified above from step u onwards using Dk,u \du; t
-  Aggregation: label-based majority vote

## Method: AmnesicML

AmesicML, 2021:
- would be to keep a record of each batch update. However, if the data holder is only concerned about possible potential removal of a subset of data, they need only keep the parameter updates from batches containing that data.
- we simply remove the parameter updates from each batch

The problem: 
- The stochasticity and incremetaly of the training process

## Method: Error-Max Anti-sample generation

Error-Max Anti-sample generation
- Class Removal
- Max instead of Min the loss
- Do some repair

错误最大化噪声生成：这一步骤的核心是生成一个噪声矩阵，这个矩阵专门用于要遗忘的类别。通过原始模型学习这个噪声矩阵，噪声矩阵被设计来最大化模型在这个特定类别上的错误率，从而有效地干扰模型对这些类别的记忆。
	2.	损害与修复步骤：
	•	损害（Impair）步骤：在这一步骤中，使用生成的噪声矩阵和非常高的学习速率对模型进行更新，以迅速减少模型对要遗忘的类别的记忆。这个过程类似于在模型的权重中注入噪声，破坏模型对特定数据的记忆。
	•	修复（Repair）步骤：在损害步骤之后，模型的整体性能可能会受到影响。修复步骤通过在不包含要遗忘类别的数据上继续训练模型来恢复性能，修复因损害步骤而受损的其他功能。


## Method: Linear Filteration

linear filtration, 2022

- Class Removal
- 应用过滤矩阵  F_z  来直接修改模型的权重  W ，生成新的权重  W_z ，
不涉及新的数据训练或额外的学习步骤


Filter 的计算

1.	计算预期预测：
对于每个类别  j ，计算该类的期望预测  \mathbf{a}_j ：

\mathbf{a}_j = \mathbb{E}[\mathbf{h}(\mathbf{x}) | Y = j]

其中， \mathbf{h}(\mathbf{x})  是分类器对输入  \mathbf{x}  的logits输出， Y = j  表示数据点属于类别  j 。
	2.	构建矩阵 A 和 B：
	•	矩阵  A  由所有类的期望预测组成，是一个  k \times k  矩阵：

A = [\mathbf{a}_0 | \mathbf{a}1 | \cdots | \mathbf{a}{k-1}]

	•	矩阵  B_z  由除了被遗忘类别外的类的变换预测构成，其中包含一个向量  z  替换被删除类别的向量：

B_z = [z | \mathbf{a}1 | \cdots | \mathbf{a}{k-1}]

	3.	计算过滤矩阵  F_z ：
过滤矩阵  F_z  是通过以下方式得到的，它将  A  转换为  B_z ：

F_z = B_z A^{-1}

这个矩阵直接作用于原始的权重矩阵  W ，从而得到新的权重矩阵  W_z ：

W_z = F_z W


为什么是F_z = B_z A^{-1}, 而不是 Bz
1.	计算预期预测：
对于每个类别  j ，计算该类的期望预测  \mathbf{a}_j ：

\mathbf{a}_j = \mathbb{E}[\mathbf{h}(\mathbf{x}) | Y = j]

其中， \mathbf{h}(\mathbf{x})  是分类器对输入  \mathbf{x}  的logits输出， Y = j  表示数据点属于类别  j 。
	2.	构建矩阵 A 和 B：
	•	矩阵  A  由所有类的期望预测组成，是一个  k \times k  矩阵：

A = [\mathbf{a}_0 | \mathbf{a}1 | \cdots | \mathbf{a}{k-1}]

	•	矩阵  B_z  由除了被遗忘类别外的类的变换预测构成，其中包含一个向量  z  替换被删除类别的向量：

B_z = [z | \mathbf{a}1 | \cdots | \mathbf{a}{k-1}]

	3.	计算过滤矩阵  F_z ：
过滤矩阵  F_z  是通过以下方式得到的，它将  A  转换为  B_z ：

F_z = B_z A^{-1}

这个矩阵直接作用于原始的权重矩阵  W ，从而得到新的权重矩阵  W_z ：

W_z = F_z W


# Continual Learning + Machine Unlearning

## How Continual Learning + Machine Unlearning?

In essence, Continual learning is a non-stationary data stream.

What's the point?

- Provide unlearning options only for this scheme. Less meaningful
- Good for continual learning itself:
  - Promote network capacity? (really? investigate MU methods) Take it as a manual approach to balance S-P trade-off

We do have proposed paradigms, though.
- 
我找到了两篇类似的把unlearning往CL靠的论文，造了两个不一样的场景:

##  Proposed Paradigm: LSF Problem

CLPU: https://lifelong-ml.cc/Conferences/2022/acceptedpapersandvideos/conf-2022-44

CLPU是在以任务为单位的，task-level，什么时候忘是根据指示，例如训练完任务1，可能在任务2，3，4训练完的时候要求1忘，也可以永远不要求


CLPU的目标是在没要求忘的任务效果尽量好（CL本身的目标），并且要求1. 训了要求忘的任务然后忘掉的结果，尽量和2.要求忘的任务从来没存在过训练的结果 尽量一致（unlearning 的目标）
Df  a task as the forgetting unit



At the k-th task, we are given the dataset Dk and the preservation set CP  k . We use index k for the new task and p for the previous tasks.  Definition 1 (LSF Problem). The Learning with Selective Forgetting (LSF) problem is defined as follows:  • Objective: Learn a model fθ : X → Y. This model fθ should map a test input x to its correct class label y if x is in the preservation set CP . Otherwise, fθ should map  x to a wrong class label y′ 6= y.  • Constraint: No original samples or generative models for the past tasks are available after the new task begins.

##  Proposed Paradigm: LSF Problem

explains that this requires users to explicitly define  which tasks will be learned permanently and which tasks will be  learned only temporarily.

Sequence tasks
Sequence Requests inserted tasks, R,T,F (by assumption in CLPU paper)
Should be R,F??


## 
![](ssets/CLPU.png)

## Metrics


## CLPU DER++

A naive method:

- Take isolated for potential task to be forgetted.
- Use R, T. setback, have to know if it can be forgetted. Should develop forget at any time.
-

## Selective Forgetting

LSF: https://www.ijcai.org/proceedings/2021/137

LSF 是在每个任务要求一部分类别是要unlearn的，class-level，每次训练新任务时都要求前面任务的unlearn set开始忘
LSF的目标是每个任务里没要求忘的类的数据效果尽量好（CL本身的目标），并且要求忘的数据效果下降的尽量厉害（unlearning 的目标）。这个目标是为了忘而忘

To the best of our knowledge, only one previous paper discusses a similar problem setting pertaining to selective forgetting in continual learning (Shibata et al., 2021). However, the problem in that paper is different from CLPU as it defines forgetting as maximally degrading the performance on a task. As discussed in Sec. 5, this requirement is not privacy-preserving and can potentially leak information (e.g., that the task has been previously learned).


-




# Next Step

## Thinking


1. 这些工作只是讨论在CL框架下搞unlearning而已，我认为单纯这样做没什么意义。
2. 我认为unlearning对continual learning本身是好处（例如释放network capacity），
3. 虽然他们指标里带着CL本身的目标，但是没有体现出来

- CL本身的目标就是在没要求忘的任务上效果好。这些工作会把带unlearning的和不带的普通的CL baseline（如EWC）进行比较，其中不带unlearning的方法就把所有任务不管要不要unlearn都一起学，然后只算没要求unlearn的任务上的平均准确率。按说，如果把所有的一起学了，肯定会挤占网络资源，导致效果不好，如果unlearning这个行为可以释放资源的话，带unlearning的算法效果应该更好的，但这个论文里的结果表格好像还不如。    
- 我猜是没有达到network capacity的极限，不需要释放capacity，即使unlearning有释放capacity的作用，也不会对效果有好的贡献。你看论文里比较的方法都是允许不够的时候向外面要capacity的，就是说capacity不是fixed而是可以扩张的，他们可以在缺的时候直接用扩张要到，不需要unlearning去释放。

 the forgotten data has to be known a priori when the original model is trained [45].
4. 所以要和解决network capacity问题这件事搭边，得放在不允许扩张的方法里面试，比如HAT，这样unlearning释放capacity然后让效果变好的作用就能体现出来了。论文里可以说既能系统性解决network capacity不足的问题，也能有效unlearning。




## next step 



- 明确目标，是unlearning的目标，还是为了持续学习好
  - 调查MU的方法
  - To design a new CL MU algorithm promoting network capacity.
- To validate, Compare the 1,2,3,F2,4,F3... fixed network average 1,2,3,4 and 1,4 after forgetting 2,3



也可以沿着他们提的这个场景去解决他们的问题，但我不看好, 它们造的。LSF场景感觉没什么意义，它忘的时候不是由我们说了算的；CLPU其实仔细看它们有个限定，它区分R和T，有的任务（标记为R的）会提前知道它之后永远不会被要求忘掉，也是个非常特殊的场景。我感觉是为了写论文或者套他们的想法方便硬造的特殊场景，没用通用性，而且指标很复杂。







## Thank You

 \centering
\Large

Thank you for your attention!

\

 Please feel free to ask any questions or reach out to me at:

[wangpengxiang@stu.pku.edu.cn](wangpengxiang@stu.pku.edu.cn)


\vfill

\begin{figure}
  \centering
  \begin{subfigure}{0.2\textwidth}
    \centering
    \includegraphics[height=0.8cm]{../assets/pku-logo.png}
  \end{subfigure}
  \hspace{0.5cm}
  \begin{subfigure}{0.2\textwidth}
    \centering
    \includegraphics[height=0.8cm]{../assets/uob-logo.png}
  \end{subfigure}
\end{figure}

